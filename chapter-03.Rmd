---
title: "Linear Methods for Regression"
output:
  html_document:
    toc: yes
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

---

## Note [p:47 eq:3.8 eq:3.10] beta-hat-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution]

[wikipedia-multivariate-normal-distribution]: https://en.wikipedia.org/wiki/Multivariate_normal_distribution

The least squares parameter estimates $\hat{\beta}$ is given by equation (3.6)

$$
\begin{equation}
\hat{\beta} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}.
\end{equation}
$$

Under the assumption that the observations $y_i$ are uncorrelated and have constant variance $\sigma^2$, and that the $x_i$ are fixed (non random), we now have

$$
\begin{equation}
E\left(\hat{\beta}\right) = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right),
\end{equation}
$$

and the variance–covariance matrix of the $\hat{\beta}$ becomes

$$
\begin{align}
Var\left(\hat{\beta}\right)
&= E\left\{ \left[\hat{\beta} - E\left(\hat{\beta}\right)\right]^T \left[\hat{\beta} - E\left(\hat{\beta}\right)\right] \right\} \\
&= E\left\{ \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right] \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right]^T \right\} \\
&= E\left[ \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \right] \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left[\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\right] \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \sigma^2I \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \sigma^2 \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2.
\end{align}
$$

Further, we assume that (3.1) is the correct model for the mean; that is, the conditional expectation of $Y$ is linear in $X_1$, ..., $X_p$, and the deviations of $Y$ around its expectation are additive and Gaussian, i.e.,

$$
\begin{equation}
Y = X^T\beta + \epsilon,
\end{equation}
$$

where the error $\epsilon$ is a Gaussian random variable with expectation zero and variance $\sigma^2$, written $\epsilon \sim N\left(0, \sigma^2\right)$. According to [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution], any linear transformation of independent standard normal random variables is a multivariate normal distribution, hence $\hat{\beta}$ is a multivariate normal distribution. The expectation of $\hat{\beta}$ can be derived straightforward as

$$
\begin{align}
E\left(\hat{\beta}\right)
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right) \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X}\beta \\
&= \beta.
\end{align}
$$

Finally we proved that:

$$
\begin{equation}
\label{eq:beta-hat-distribution}
\hat{\beta} \sim N\left(\beta, \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2\right).
\end{equation}
$$

---

## Note [p:47 eq:3.11] training-rss-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Idempotent matrix][wikipedia-idempotent-matrix], [Wikipedia: Positive-definite matrix][wikipedia-positive-definite-matrix], [Wikipedia: Spectral theorem][wikipedia-spectral-theorem], [Wikipedia: Chi-squared distribution][wikipedia-chi-squared-distribution]

[wikipedia-idempotent-matrix]: https://en.wikipedia.org/wiki/Idempotent_matrix
[wikipedia-positive-definite-matrix]: https://en.wikipedia.org/wiki/Positive-definite_matrix
[wikipedia-spectral-theorem]: https://en.wikipedia.org/wiki/Spectral_theorem
[wikipedia-chi-squared-distribution]: https://en.wikipedia.org/wiki/Chi-squared_distribution

In the solution to **Ex. 2.9**, we got the representation of training RSS by the hat matrix

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \left(\mathbf{y} - \hat{\mathbf{y}}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\hat{\beta}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \left(\mathbf{X}\beta + \mathbf{\epsilon}\right)\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\beta - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right) \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon},
\end{align}
$$

where $H$ is:

1. symmetric: $H^T = H$;
1. [idempotent][wikipedia-idempotent-matrix]: $HH = H$; and
1. [positive semi-definite][wikipedia-positive-definite-matrix]: $z^THz = z^TH^THz = \left(Hz\right)^T\left(Hz\right) \ge 0$.

[Eigenvalues of idempotent matrices are either 0 or 1](http://everything2.com/title/Eigenvalues+of+idempotent+matrices+are+either+0+or+1): For any idempotent matrix $A$,

$$
\begin{equation}
\lambda v = Av = A^2v = A\left(Av\right) = A\lambda v = \lambda Av = \lambda^2 v.
\end{equation}
$$

$H$ is also a normal matrix ($H^{\ast}H = HH^{\ast}$), then it is can be unitary diagonalized according to the [Spectral theorem][wikipedia-spectral-theorem]:

$$
\begin{equation}
H = UDU^{\ast},
\end{equation}
$$

where $U$ is a real unitary matrix and $U^{\ast} = U^{-1}= U^T$, and $D$ is a diagonal matrix with eigenvalues appearing at the diagonal entries. Under appropriate permutations, we can arrange the eigenvalues of $1$s before all the $0$s.

Then

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^TUU^T\mathbf{\epsilon} - \mathbf{\epsilon}^TUDU^T\mathbf{\epsilon} \\
&= \left(U^T\mathbf{\epsilon}\right)^T\left(U^T\mathbf{\epsilon}\right) - \left(U^T\mathbf{\epsilon}\right)^TD\left(U^T\mathbf{\epsilon}\right) \\
&= \mathbf{z}^T\mathbf{z} - \mathbf{z}^TD\mathbf{z} \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p+1}^2\right) \\
&= z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2,
\end{align}
$$

where $z_i = \left<u_i, \mathbf{\epsilon}\right>$ is the inner product between the $i$th column of $U$ and $\mathbf{\epsilon}$. It is easy to show that all the $z_i$ follow Gaussian distributions with variance $\sigma^2$ (norms of $u_i$ are 1) and are independent of each other (columns of $U$ are orthogonal of each other), in other words, $RSS\left(\hat{\beta}\right)$ is a sum of squared $N-p-1$ independent normal variables. By definition, $RSS\left(\hat{\beta}\right)/\sigma^2$ follows a [Chi-squared distribution][wikipedia-chi-squared-distribution]:

$$
\begin{equation}
\label{eq:rss-distribution}
\frac{RSS\left(\hat{\beta}\right)}{\sigma^2} \sim \chi_{N-p-1}^2,
\end{equation}
$$

or in another form

$$
\begin{equation}
\left(N-p-1\right)\hat{\sigma}^2 \sim \sigma^2\chi_{N-p-1}^2.
\end{equation}
$$

---

## Note [p:48 eq:3.12] t-statistic

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/26
Reference | [Wikipedia: Student's t-distribution][wikipedia-student's-t-distribution], [Wikipedia: Student's t-test][wikipedia-student's-t-test]

[wikipedia-student's-t-distribution]: https://en.wikipedia.org/wiki/Student's_t-distribution
[wikipedia-student's-t-test]: https://en.wikipedia.org/wiki/Student's_t-test

Here we are going to prove that _Z-score_

$$
\begin{equation}
z_j = \frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}},
\end{equation}
$$

where $v_j$ is the $j$th diagonal element of $\left(\mathbf{X}^T\mathbf{X}\right)^{−1}$, follows a Student's $t$-distribution under the null hypothesis that $\beta_j = 0$.

Recall the distrubution of $\hat{\beta}$ in $\eqref{eq:beta-hat-distribution}$, it is easy to show that $\frac{\hat{\beta_j}-0}{\sigma\sqrt{v_j}}$ follows the standard normal distribution. And as shown in $\eqref{eq:rss-distribution}$, $\frac{RSS\left(\hat{\beta}\right)}{\sigma^2}$ follows the chi-squared distribution with $N-p-1$ degrees of freedom, therefore if we can proof that $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are independent of each other then by the definition of [Student's t-distribution][wikipedia-student's-t-distribution],

$$
\begin{equation}
z_j = \frac{\frac{\hat{\beta_j}-0}{\sigma\sqrt{v_j}}}{\sqrt{\frac{RSS\left(\hat{\beta}\right)}{\sigma^2 \left(N-p-1\right)}}} = \frac{\hat{\beta_j}}{\sqrt{\frac{RSS\left(\hat{\beta}\right)}{N-p-1}}\sqrt{v_j}} = \frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}}
\end{equation}
$$

follows the $t$ distribution with $N−p−1$ degrees of freedom.

We now proceed to proof $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are independent of each other by making use of QR decomposition of $\mathbf{X}$ as shown in (3.31), (3.31) and (3.32)

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R}, \\
\hat{\beta} &= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}, \\
\hat{y} &= \mathbf{Q}\mathbf{Q}^T\mathbf{y},
\end{align}
$$

where $\mathbf{Q}$ is an $N\times\left(p+1\right)$ orthogonal matrix, $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$, and $\mathbf{R}$ is a $\left(p+1\right)\times\left(p+1\right)$ upper triangular matrix.

We make up a $N\times\left(N-p-1\right)$ orthogonal matrix $\mathbf{Q}_0$ satisfying that all columns of $\mathbf{Q}$ and $\mathbf{Q}_0$ are orthogonal to each other, in other words the $N \times N$ matrix $\mathbf{U}$ is a (real) unitary matrix:

$$
\begin{equation}
\mathbf{U} = \left[ \begin{array}{cc} \mathbf{Q} & \mathbf{Q}_0 \end{array} \right].
\end{equation}
$$

In the following derivations, we denote by $z_i = \left<u_i, \mathbf{\epsilon}\right>$ the inner product between the $i$th column of $\mathbf{U}$ and $\mathbf{\epsilon}$. First we look at $\hat{\beta}$

$$
\begin{align}
\hat{\beta}
&= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y} \\
&= \mathbf{R}^{-1}\mathbf{Q}^T\left(\mathbf{X}\beta+\mathbf{\epsilon}\right) \\
&= \beta + \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{\epsilon}.
\end{align}
$$

We can see that $\hat{\beta}$ is a linear combination of $z_1$, $z_2$, ..., and $z_{p+1}$.

Then what about $RSS\left(\hat{\beta}\right)$?

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}\mathbf{Q}^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p+1}^2\right) \\
&= z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2.
\end{align}
$$

Then $RSS\left(\hat{\beta}\right)$ is a function of $z_{p+2}$, $z_{p+3}$, ..., and $z_N$. Because $\epsilon_i$ are independent standard normal random variables and $\mathbf{U}$ is unitary, then $z_i$ are independent standard normal random variables, too. Finally we proved that $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are indenpendent of each other.

