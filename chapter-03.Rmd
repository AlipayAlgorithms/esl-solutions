---
title: "Linear Methods for Regression"
output:
  html_document:
    toc: yes
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

---

## Note [p:47 eq:3.8 eq:3.10] beta-hat-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution]

[wikipedia-multivariate-normal-distribution]: https://en.wikipedia.org/wiki/Multivariate_normal_distribution

The least squares parameter estimates $\hat{\beta}$ is given by equation (3.6)

$$
\begin{equation}
\hat{\beta} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}.
\end{equation}
$$

Under the assumption that the observations $y_i$ are uncorrelated and have constant variance $\sigma^2$, and that the $x_i$ are fixed (non random), we now have

$$
\begin{equation}
E\left(\hat{\beta}\right) = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right),
\end{equation}
$$

and the varianceâ€“covariance matrix of the $\hat{\beta}$ becomes

$$
\begin{align}
Var\left(\hat{\beta}\right)
&= E\left\{ \left[\hat{\beta} - E\left(\hat{\beta}\right)\right]^T \left[\hat{\beta} - E\left(\hat{\beta}\right)\right] \right\} \\
&= E\left\{ \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right] \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right]^T \right\} \\
&= E\left[ \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \right] \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left[\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\right] \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \sigma^2I \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \sigma^2 \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2.
\end{align}
$$

Further, we assume that (3.1) is the correct model for the mean; that is, the conditional expectation of $Y$ is linear in $X_1$, ..., $X_p$, and the deviations of $Y$ around its expectation are additive and Gaussian, i.e.,

$$
\begin{equation}
Y = X^T\beta + \epsilon,
\end{equation}
$$

where the error $\epsilon$ is a Gaussian random variable with expectation zero and variance $\sigma^2$, written $\epsilon \sim N\left(0, \sigma^2\right)$. According to [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution], any linear transformation of independent standard normal random variables is a multivariate normal distribution, hence $\hat{\beta}$ is a multivariate normal distribution. The expectation of $\hat{\beta}$ can be derived straightforward as

$$
\begin{align}
E\left(\hat{\beta}\right)
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right) \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X}\beta \\
&= \beta.
\end{align}
$$

Finally we proved that:

$$
\begin{equation}
\hat{\beta} \sim N\left(\beta, \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2\right).
\end{equation}
$$

## Note [p:47 eq:3.11] training-rss-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Idempotent matrix][wikipedia-idempotent-matrix], [Wikipedia: Positive-definite matrix][wikipedia-positive-definite-matrix], [Wikipedia: Spectral theorem][wikipedia-spectral-theorem], [Wikipedia: Chi-squared distribution][wikipedia-chi-squared-distribution]

[wikipedia-idempotent-matrix]: https://en.wikipedia.org/wiki/Idempotent_matrix
[wikipedia-positive-definite-matrix]: https://en.wikipedia.org/wiki/Positive-definite_matrix
[wikipedia-spectral-theorem]: https://en.wikipedia.org/wiki/Spectral_theorem
[wikipedia-chi-squared-distribution]: https://en.wikipedia.org/wiki/Chi-squared_distribution

In the solution to **Ex. 2.9**, we got the representation of training RSS by the hat matrix

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \left(\mathbf{y} - \hat{\mathbf{y}}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\hat{\beta}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \left(\mathbf{X}\beta + \mathbf{\epsilon}\right)\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\beta - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right) \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon},
\end{align}
$$

where $H$ is:

1. symmetric: $H^T = H$;
1. [idempotent][wikipedia-idempotent-matrix]: $HH = H$; and
1. [positive semi-definite][wikipedia-positive-definite-matrix]: $z^THz = z^TH^THz = \left(Hz\right)^T\left(Hz\right) \ge 0$.

[Eigenvalues of idempotent matrices are either 0 or 1](http://everything2.com/title/Eigenvalues+of+idempotent+matrices+are+either+0+or+1): For any idempotent matrix $A$,

$$
\begin{equation}
\lambda v = Av = A^2v = A\left(Av\right) = A\lambda v = \lambda Av = \lambda^2 v.
\end{equation}
$$

$H$ is also a normal matrix ($H^{\ast}H = HH^{\ast}$), then it is can be unitary diagonalized according to the [Spectral theorem][wikipedia-spectral-theorem]:

$$
\begin{equation}
H = UDU^{\ast},
\end{equation}
$$

where $U$ is a real unitary matrix and $U^{\ast} = U^{-1}= U^T$, and $D$ is a diagonal matrix with eigenvalues appearing at the diagonal entries. Under appropriate permutations, we can arrange the eigenvalues of $1$s before all the $0$s.

Then

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^TUU^T\mathbf{\epsilon} - \mathbf{\epsilon}^TUDU^T\mathbf{\epsilon} \\
&= \left(U^T\mathbf{\epsilon}\right)^T\left(U^T\mathbf{\epsilon}\right) - \left(U^T\mathbf{\epsilon}\right)^TD\left(U^T\mathbf{\epsilon}\right) \\
&= \mathbf{z}^T\mathbf{z} - \mathbf{z}^TD\mathbf{z} \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_p^2\right) \\
&= \left(z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2\right),
\end{align}
$$

where $z_i = \left<u_i, \mathbf{\epsilon}\right>$. It is easy to show that all the $z_i$ follow Gaussian distributions with variance $\sigma^2$ (norms of $u_i$ are 1) and are independent of each other (columns of $U$ are orthogonal of each other), in other words, $RSS\left(\hat{\beta}\right)$ is a sum of squared $N-p-1$ independent normal variables. By definition, $RSS\left(\hat{\beta}\right)/\sigma^2$ follows a [Chi-squared distribution][wikipedia-chi-squared-distribution]:

$$
\begin{equation}
\frac{RSS\left(\hat{\beta}\right)}{\sigma^2} \sim \chi_{N-p-1}^2,
\end{equation}
$$

or in another form

$$
\begin{equation}
\left(N-p-1\right)\hat{\sigma}^2 \sim \sigma^2\chi_{N-p-1}^2.
\end{equation}
$$

