---
title: "Linear Methods for Regression"
output:
  html_document:
    toc: yes
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

---

## Note [p:47 eq:3.8 eq:3.10] beta-hat-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution]

[wikipedia-multivariate-normal-distribution]: https://en.wikipedia.org/wiki/Multivariate_normal_distribution

The least squares parameter estimates $\hat{\beta}$ is given by equation (3.6)

$$
\begin{equation}
\hat{\beta} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}.
\end{equation}
$$

Under the assumption that the observations $y_i$ are uncorrelated and have constant variance $\sigma^2$, and that the $x_i$ are fixed (non random), we now have

$$
\begin{equation}
E\left(\hat{\beta}\right) = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right),
\end{equation}
$$

and the variance–covariance matrix of the $\hat{\beta}$ becomes

$$
\begin{align}
Var\left(\hat{\beta}\right)
&= E\left\{ \left[\hat{\beta} - E\left(\hat{\beta}\right)\right] \left[\hat{\beta} - E\left(\hat{\beta}\right)\right]^T \right\} \\
&= E\left\{ \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right] \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right]^T \right\} \\
&= E\left[ \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \right] \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left[\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\right] \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \sigma^2I \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \sigma^2 \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2.
\end{align}
$$

Further, we assume that (3.1) is the correct model for the mean; that is, the conditional expectation of $Y$ is linear in $X_1$, ..., $X_p$, and the deviations of $Y$ around its expectation are additive and Gaussian, i.e.,

$$
\begin{equation}
Y = X^T\beta + \epsilon,
\end{equation}
$$

where the error $\epsilon$ is a Gaussian random variable with expectation zero and variance $\sigma^2$, written $\epsilon \sim N\left(0, \sigma^2\right)$. According to [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution], any linear transformation of independent standard normal random variables is a multivariate normal distribution, hence $\hat{\beta}$ is a multivariate normal distribution. The expectation of $\hat{\beta}$ can be derived straightforward as

$$
\begin{align}
E\left(\hat{\beta}\right)
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right) \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X}\beta \\
&= \beta.
\end{align}
$$

Finally we proved that:

$$
\begin{equation}
\label{eq:beta-hat-distribution}
\hat{\beta} \sim N\left(\beta, \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2\right).
\end{equation}
$$

---

## Note [p:47 eq:3.11] training-rss-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Idempotent matrix][wikipedia-idempotent-matrix], [Wikipedia: Positive-definite matrix][wikipedia-positive-definite-matrix], [Wikipedia: Spectral theorem][wikipedia-spectral-theorem], [Wikipedia: Chi-squared distribution][wikipedia-chi-squared-distribution]

[wikipedia-idempotent-matrix]: https://en.wikipedia.org/wiki/Idempotent_matrix
[wikipedia-positive-definite-matrix]: https://en.wikipedia.org/wiki/Positive-definite_matrix
[wikipedia-spectral-theorem]: https://en.wikipedia.org/wiki/Spectral_theorem
[wikipedia-chi-squared-distribution]: https://en.wikipedia.org/wiki/Chi-squared_distribution

In the solution to **Ex. 2.9**, we got the representation of training RSS by the hat matrix

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \left(\mathbf{y} - \hat{\mathbf{y}}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\hat{\beta}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \left(\mathbf{X}\beta + \mathbf{\epsilon}\right)\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\beta - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right) \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon},
\end{align}
$$

where $H$ is:

1. symmetric: $H^T = H$;
1. [idempotent][wikipedia-idempotent-matrix]: $HH = H$; and
1. [positive semi-definite][wikipedia-positive-definite-matrix]: $z^THz = z^TH^THz = \left(Hz\right)^T\left(Hz\right) \ge 0$.

[Eigenvalues of idempotent matrices are either 0 or 1](http://everything2.com/title/Eigenvalues+of+idempotent+matrices+are+either+0+or+1): For any idempotent matrix $A$,

$$
\begin{equation}
\lambda v = Av = A^2v = A\left(Av\right) = A\lambda v = \lambda Av = \lambda^2 v.
\end{equation}
$$

$H$ is also a normal matrix ($H^{\ast}H = HH^{\ast}$), then it can be unitary diagonalized according to the [Spectral theorem][wikipedia-spectral-theorem]:

$$
\begin{equation}
H = UDU^{\ast},
\end{equation}
$$

where $U$ is a real unitary matrix and $U^{\ast} = U^T = U^{-1}$, and $D$ is a diagonal matrix with eigenvalues appearing at the diagonal entries. Under appropriate permutations, we can rearrange the eigenvalues of $1$s before all the $0$s.

Then

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^TUU^T\mathbf{\epsilon} - \mathbf{\epsilon}^TUDU^T\mathbf{\epsilon} \\
&= \left(U^T\mathbf{\epsilon}\right)^T\left(U^T\mathbf{\epsilon}\right) - \left(U^T\mathbf{\epsilon}\right)^TD\left(U^T\mathbf{\epsilon}\right) \\
&= \mathbf{z}^T\mathbf{z} - \mathbf{z}^TD\mathbf{z} \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p+1}^2\right) \\
&= z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2,
\end{align}
$$

where $z_i = \left<u_i, \mathbf{\epsilon}\right>$ is the inner product between the $i$th column of $U$ and $\mathbf{\epsilon}$. It is easy to show that all the $z_i$ follow Gaussian distributions with variance $\sigma^2$ (norms of $u_i$ are 1) and are independent of each other (columns of $U$ are orthogonal of each other), in other words, $RSS\left(\hat{\beta}\right)$ is a sum of squared $N-p-1$ independent normal variables. By definition, $\left. RSS\middle(\hat{\beta}\middle)\middle/\sigma^2 \right.$ follows a [Chi-squared distribution][wikipedia-chi-squared-distribution]:

$$
\begin{equation}
\label{eq:rss-distribution}
\frac{RSS\left(\hat{\beta}\right)}{\sigma^2} \sim \chi_{N-p-1}^2,
\end{equation}
$$

or in another form

$$
\begin{equation}
\left(N-p-1\right)\hat{\sigma}^2 \sim \sigma^2\chi_{N-p-1}^2.
\end{equation}
$$

---

## Note [p:48 eq:3.12] t-statistic

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/26
Reference | [Wikipedia: Student's t-distribution][wikipedia-student's-t-distribution], [Wikipedia: Student's t-test][wikipedia-student's-t-test]

[wikipedia-student's-t-distribution]: https://en.wikipedia.org/wiki/Student's_t-distribution
[wikipedia-student's-t-test]: https://en.wikipedia.org/wiki/Student's_t-test

Here we are going to prove that _Z-score_

$$
\begin{equation}
z_j = \frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}},
\end{equation}
$$

where $v_j$ is the $j$th diagonal element of $\left(\mathbf{X}^T\mathbf{X}\right)^{−1}$, follows a Student's $t$-distribution under the null hypothesis that $\beta_j = 0$.

Recall the distrubution of $\hat{\beta}$ in $\eqref{eq:beta-hat-distribution}$, it is easy to show that $\frac{\hat{\beta_j}-0}{\sigma\sqrt{v_j}}$ follows the standard normal distribution. And as shown in $\eqref{eq:rss-distribution}$, $\frac{RSS\left(\hat{\beta}\right)}{\sigma^2}$ follows the chi-squared distribution with $N-p-1$ degrees of freedom, therefore if we can proof that $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are independent of each other then by the definition of [Student's t-distribution][wikipedia-student's-t-distribution],

$$
\begin{equation}
z_j = \frac{\frac{\hat{\beta_j}-0}{\sigma\sqrt{v_j}}}{\sqrt{\frac{RSS\left(\hat{\beta}\right)}{\sigma^2 \left(N-p-1\right)}}} = \frac{\hat{\beta_j}}{\sqrt{\frac{RSS\left(\hat{\beta}\right)}{N-p-1}}\sqrt{v_j}} = \frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}}
\end{equation}
$$

follows the $t$ distribution with $N−p−1$ degrees of freedom.

We now proceed to proof $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are independent of each other by making use of **QR** decomposition of $\mathbf{X}$ as shown in (3.31), (3.31) and (3.32)

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R}, \\
\hat{\beta} &= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}, \\
\hat{y} &= \mathbf{Q}\mathbf{Q}^T\mathbf{y},
\end{align}
$$

where $\mathbf{Q}$ is an $N\times\left(p+1\right)$ orthogonal matrix, $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$, and $\mathbf{R}$ is a $\left(p+1\right)\times\left(p+1\right)$ upper triangular matrix.

We make up a $N\times\left(N-p-1\right)$ orthogonal matrix $\mathbf{Q}_r$ satisfying that all columns of $\mathbf{Q}$ and $\mathbf{Q}_r$ are orthogonal to each other, in other words the $N \times N$ matrix $\mathbf{U}$ is a (real) unitary matrix:

$$
\begin{equation}
\label{eq:q-complemented-as-u}
\mathbf{U} = \left[ \begin{array}{cc} \mathbf{Q} & \mathbf{Q}_r \end{array} \right].
\end{equation}
$$

In the following derivations, we denote by $z_i = \left<u_i, \mathbf{\epsilon}\right>$ the inner product between the $i$th column of $\mathbf{U}$ and $\mathbf{\epsilon}$. First we look at $\hat{\beta}$:

$$
\begin{align}
\hat{\beta}
&= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y} \\
&= \mathbf{R}^{-1}\mathbf{Q}^T\left(\mathbf{X}\beta+\mathbf{\epsilon}\right) \\
&= \beta + \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{\epsilon}.
\end{align}
$$

We can see that $\hat{\beta}$ is a function of $z_1$, $z_2$, ..., and $z_{p+1}$.

What about $RSS\left(\hat{\beta}\right)$?

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}\mathbf{Q}^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p+1}^2\right) \\
&= z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2.
\end{align}
$$

Then $RSS\left(\hat{\beta}\right)$ is a function of $z_{p+2}$, $z_{p+3}$, ..., and $z_N$. Because $\epsilon_i$ are independent normal random variables and $\mathbf{U}$ is unitary, then $z_i$ are independent normal random variables, too. Finally we proved that $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are indenpendent of each other.

---

## Note [p:48 eq:3.13] f-statistic

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/26
Reference | [Wikipedia: F-distribution][wikipedia-f-distribution], [Wikipedia: F-test][wikipedia-f-test]

[wikipedia-f-distribution]: https://en.wikipedia.org/wiki/F-distribution
[wikipedia-f-test]: https://en.wikipedia.org/wiki/F-test

In (3.13) the $F$ statistic is given as

$$
\begin{equation}
\label{eq:f-statistic}
F = \frac{\left. \middle(RSS_0-RSS_1\middle) \middle/ \middle(p_1-p_0\middle) \right.}{\left. RSS_1 \middle/ \middle(N-p_1-1\middle) \right.},
\end{equation}
$$

where $RSS_1$ is the residual sum-of-squares for the least squares fit of the bigger model with $p_1+1$ parameters, and $RSS_0$ the same for the nested smaller model with $p_0+1$ parameters, having $p_1−p_0$ parameters constrained to be zero.

Here we are going to prove that $F$ statistic has a $F_{p_1−p_0,N−p_1−1}$ distribution, under the Gaussian assumptions, and the null hypothesis that the smaller model is correct.

Let's rearrange the cloumns in $\mathbf{X}$ so that the column vectors used by the larger model only appearing at the rightmost. Appling **QR** decomposition we get

$$
\begin{align}
\mathbf{X}
&= \mathbf{Q}\mathbf{R} \\
&= \left[ \begin{array}{cc} \mathbf{Q}_0 & \mathbf{Q}_s \end{array} \right] \left[ \begin{array}{cc} \mathbf{R}_0 & \mathbf{R}_{s1} \\ \mathbf{0} & \mathbf{R}_{s2} \end{array} \right], \\
\mathbf{X}_0 &= \mathbf{Q}_0\mathbf{R}_0,
\end{align}
$$

where $\mathbf{Q}_0$ is a $N \times \left(p_0+1\right)$ orthogonal matrix and $\mathbf{R}_0$
is a $(p_0+1) \times (p_0+1)$ upper triangular matrix. Then the RSSs can be simplified as

$$
\begin{align}
RSS_1
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}\mathbf{Q}^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p_1+1}^2\right) \\
&= z_{p_1+2}^2 + z_{p_1+3}^2 + \dots + z_N^2, \\
RSS_0
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH_0\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}_0\mathbf{Q}_0^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}_0^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}_0^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p_0+1}^2\right) \\
&= z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_N^2, \\
RSS_0 - RSS_1
&= \left(z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_N^2\right) - \left(z_{p_1+2}^2 + z_{p_1+3}^2 + \dots + z_N^2\right) \\
&= z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_{p_1+1}^2,
\end{align}
$$

where $\mathbf{U}$ is defined as $\eqref{eq:q-complemented-as-u}$.

Because $\epsilon_i$ are independent normal random variables and $\mathbf{U}$ is unitary, then $z_i$ are independent normal random variables, too. Then we have $\frac{RSS_1}{\sigma^2} \sim \chi_{N-p_1-1}^2$, $\frac{RSS_0-RSS_1}{\sigma^2} \sim \chi_{p_1-p_0}^2$, and those two are independent of each other. By the definition of [F-distribution][wikipedia-f-distribution], the $F$ statistic in $\eqref{eq:f-statistic}$ follows the $F$ distribution with $p_1-p_0$ and $N−p_1−1$ degrees of freedom respectively.

---

## Ex. 3.1 f-statistic-and-z-score

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/28
Reference | [Weatherwax and Epstein's solution][weatherwax-epstein-solution], [Wikipedia: Triangular matrix][wikipedia-triangular-matrix]

[weatherwax-epstein-solution]: http://waxworksmath.com/Authors/G_M/Hastie/WriteUp/weatherwax_epstein_hastie_solutions_manual.pdf
[wikipedia-triangular-matrix]: https://en.wikipedia.org/wiki/Triangular_matrix

**Ex. 3.1** Show that the $F$ statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding $z$-score (3.12).

**Proof**

Our proof is inspired by [Weatherwax and Epstein's solution][weatherwax-epstein-solution] whereas ours is more compact.

Rearrange the columns of $\mathbf{X}$ by putting $\mathbf{x_j}$ to the last position, under **QR** decomposition

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R}, \\
\hat{\beta} &= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}, \\
\hat{y} &= \mathbf{Q}\mathbf{Q}^T\mathbf{y},
\end{align}
$$

the error reduction by indroducing the new variable $X_j$ is

$$
\begin{equation}
RSS_0-RSS_1 = \left<q_j, \mathbf{y}\right>.
\end{equation}
$$

This fact is well explained in **Algorithm 3.1** on page 54.

$\mathbf{R}$ is an [upper triangular matrix][wikipedia-triangular-matrix], the inverse of it
$\mathbf{R}^{-1}$ is also an upper triangular matrix, and the diagonal entries of $\mathbf{R}^{-1}$ are reciprocal of the corresponding entries in $\mathbf{R}$. $\mathbf{R}^T$ and $\left(\mathbf{R}^T\right)^{-1}$ are hence lower triangular matrices.

It can be seen that

$$
\begin{equation}
\hat{\beta_j}^2 = \left<q_j,\mathbf{y}\right> \left/\right. r_{jj}^2,
\end{equation}
$$

where $r_{jj}$ is the $j$th (the last) diagonal entries of $\mathbf{R}$.

Because

$$
\begin{align}
\mathbf{X}^T\mathbf{X}
&= \mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{R} = \mathbf{R}^T\mathbf{R}, \\
\left(\mathbf{X}^T\mathbf{X}\right)^{-1}
&= \mathbf{R}^{-1}\left(\mathbf{R}^T\right)^{-1},
\end{align}
$$

$v_j$, the $j$th (the last) diagonal element of $\left(\mathbf{X}^T\mathbf{X}\right)^{-1}$, will be $1 \left/\right. r_{jj}^2$.

Consequently,

$$
\begin{align}
z_j^2
&= \left(\frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}}\right)^2
= \frac{\hat{\beta_j}^2}{\hat{\sigma}^2v_j}
= \frac{\left<q_j,\mathbf{y}\right> \left/\right. r_{jj}^2}{\hat{\sigma}^2\left(1 \left/\right. r_{jj}^2\right)}
= \frac{\left<q_j,\mathbf{y}\right>}{\hat{\sigma}^2}, \\
F
&= \frac{\left. \middle(RSS_0-RSS_1\middle) \middle/ \middle(p_1-p_0\middle) \right.}{\left. RSS_1 \middle/ \middle(N-p_1-1\middle) \right.}
= \frac{\left<q_j, \mathbf{y}\right>}{\hat{\sigma}^2};
\end{align}
$$

Q.E.D.

