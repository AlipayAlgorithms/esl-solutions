---
title: "Linear Methods for Regression"
output:
  html_document:
    toc: yes
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

---

## Note [p:47 eq:3.8 eq:3.10] beta-hat-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution]

[wikipedia-multivariate-normal-distribution]: https://en.wikipedia.org/wiki/Multivariate_normal_distribution

The least squares parameter estimates $\hat{\beta}$ is given by equation (3.6)

$$
\begin{equation}
\hat{\beta} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}.
\end{equation}
$$

Under the assumption that the observations $y_i$ are uncorrelated and have constant variance $\sigma^2$, and that the $x_i$ are fixed (non random), we now have

$$
\begin{equation}
E\left(\hat{\beta}\right) = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right),
\end{equation}
$$

and the variance–covariance matrix of the $\hat{\beta}$ becomes

$$
\begin{align}
Var\left(\hat{\beta}\right)
&= E\left\{ \left[\hat{\beta} - E\left(\hat{\beta}\right)\right] \left[\hat{\beta} - E\left(\hat{\beta}\right)\right]^T \right\} \\
&= E\left\{ \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right] \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right]^T \right\} \\
&= E\left[ \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \right] \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left[\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\right] \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \sigma^2I \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \sigma^2 \\
\label{eq:note-beta-hat-variance}
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2.
\end{align}
$$

Further, we assume that (3.1) is the correct model for the mean; that is, the conditional expectation of $Y$ is linear in $X_1$, ..., $X_p$, and the deviations of $Y$ around its expectation are additive and Gaussian, i.e.,

$$
\begin{equation}
Y = X^T\beta + \epsilon,
\end{equation}
$$

where the error $\epsilon$ is a Gaussian random variable with expectation zero and variance $\sigma^2$, written $\epsilon \sim N\left(0, \sigma^2\right)$. According to [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution], any linear transformation of independent standard normal random variables is a multivariate normal distribution, hence $\hat{\beta}$ is a multivariate normal distribution. The expectation of $\hat{\beta}$ can be derived straightforward as

$$
\begin{align}
E\left(\hat{\beta}\right)
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right) \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X}\beta \\
&= \beta.
\end{align}
$$

Finally we proved that:

$$
\begin{equation}
\label{eq:beta-hat-distribution}
\hat{\beta} \sim N\left(\beta, \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2\right).
\end{equation}
$$

---

## Note [p:47 eq:3.11] training-rss-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Idempotent matrix][wikipedia-idempotent-matrix], [Wikipedia: Positive-definite matrix][wikipedia-positive-definite-matrix], [Wikipedia: Spectral theorem][wikipedia-spectral-theorem], [Wikipedia: Chi-squared distribution][wikipedia-chi-squared-distribution]

[wikipedia-idempotent-matrix]: https://en.wikipedia.org/wiki/Idempotent_matrix
[wikipedia-positive-definite-matrix]: https://en.wikipedia.org/wiki/Positive-definite_matrix
[wikipedia-spectral-theorem]: https://en.wikipedia.org/wiki/Spectral_theorem
[wikipedia-chi-squared-distribution]: https://en.wikipedia.org/wiki/Chi-squared_distribution

In the solution to **Ex. 2.9**, we got the representation of training RSS by the hat matrix

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \left(\mathbf{y} - \hat{\mathbf{y}}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\hat{\beta}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \left(\mathbf{X}\beta + \mathbf{\epsilon}\right)\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\beta - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right) \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon},
\end{align}
$$

where $H$ is:

1. symmetric: $H^T = H$;
1. [idempotent][wikipedia-idempotent-matrix]: $HH = H$; and
1. [positive semi-definite][wikipedia-positive-definite-matrix]: $z^THz = z^TH^THz = \left(Hz\right)^T\left(Hz\right) \ge 0$.

[Eigenvalues of idempotent matrices are either 0 or 1](http://everything2.com/title/Eigenvalues+of+idempotent+matrices+are+either+0+or+1): For any idempotent matrix $A$,

$$
\begin{equation}
\lambda v = Av = A^2v = A\left(Av\right) = A\lambda v = \lambda Av = \lambda^2 v.
\end{equation}
$$

$H$ is also a normal matrix ($H^{\ast}H = HH^{\ast}$), then it can be unitary diagonalized according to the [Spectral theorem][wikipedia-spectral-theorem]:

$$
\begin{equation}
H = UDU^{\ast},
\end{equation}
$$

where $U$ is a real unitary matrix and $U^{\ast} = U^T = U^{-1}$, and $D$ is a diagonal matrix with eigenvalues appearing at the diagonal entries. Under appropriate permutations, we can rearrange the eigenvalues of $1$s before all the $0$s.

Then

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^TUU^T\mathbf{\epsilon} - \mathbf{\epsilon}^TUDU^T\mathbf{\epsilon} \\
&= \left(U^T\mathbf{\epsilon}\right)^T\left(U^T\mathbf{\epsilon}\right) - \left(U^T\mathbf{\epsilon}\right)^TD\left(U^T\mathbf{\epsilon}\right) \\
&= \mathbf{z}^T\mathbf{z} - \mathbf{z}^TD\mathbf{z} \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p+1}^2\right) \\
&= z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2,
\end{align}
$$

where $z_i = \left<u_i, \mathbf{\epsilon}\right>$ is the inner product between the $i$th column of $U$ and $\mathbf{\epsilon}$. It is easy to show that all the $z_i$ follow Gaussian distributions with variance $\sigma^2$ (norms of $u_i$ are 1) and are independent of each other (columns of $U$ are orthogonal of each other), in other words, $RSS\left(\hat{\beta}\right)$ is a sum of squared $N-p-1$ independent normal variables. By definition, $\left. RSS\middle(\hat{\beta}\middle)\middle/\sigma^2 \right.$ follows a [Chi-squared distribution][wikipedia-chi-squared-distribution]:

$$
\begin{equation}
\label{eq:rss-distribution}
\frac{RSS\left(\hat{\beta}\right)}{\sigma^2} \sim \chi_{N-p-1}^2,
\end{equation}
$$

or in another form

$$
\begin{equation}
\left(N-p-1\right)\hat{\sigma}^2 \sim \sigma^2\chi_{N-p-1}^2.
\end{equation}
$$

---

## Note [p:48 eq:3.12] t-statistic

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/26
Reference | [Wikipedia: Student's t-distribution][wikipedia-student's-t-distribution], [Wikipedia: Student's t-test][wikipedia-student's-t-test]

[wikipedia-student's-t-distribution]: https://en.wikipedia.org/wiki/Student's_t-distribution
[wikipedia-student's-t-test]: https://en.wikipedia.org/wiki/Student's_t-test

Here we are going to prove that _Z-score_

$$
\begin{equation}
z_j = \frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}},
\end{equation}
$$

where $v_j$ is the $j$th diagonal element of $\left(\mathbf{X}^T\mathbf{X}\right)^{−1}$, follows a Student's $t$-distribution under the null hypothesis that $\beta_j = 0$.

Recall the distrubution of $\hat{\beta}$ in $\eqref{eq:beta-hat-distribution}$, it is easy to show that $\frac{\hat{\beta_j}-0}{\sigma\sqrt{v_j}}$ follows the standard normal distribution. And as shown in $\eqref{eq:rss-distribution}$, $\frac{RSS\left(\hat{\beta}\right)}{\sigma^2}$ follows the chi-squared distribution with $N-p-1$ degrees of freedom, therefore if we can proof that $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are independent of each other then by the definition of [Student's t-distribution][wikipedia-student's-t-distribution],

$$
\begin{equation}
z_j = \frac{\frac{\hat{\beta_j}-0}{\sigma\sqrt{v_j}}}{\sqrt{\frac{RSS\left(\hat{\beta}\right)}{\sigma^2 \left(N-p-1\right)}}} = \frac{\hat{\beta_j}}{\sqrt{\frac{RSS\left(\hat{\beta}\right)}{N-p-1}}\sqrt{v_j}} = \frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}}
\end{equation}
$$

follows the $t$ distribution with $N−p−1$ degrees of freedom.

We now proceed to proof $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are independent of each other by making use of **QR** decomposition of $\mathbf{X}$ as shown in (3.31), (3.31) and (3.32)

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R}, \\
\hat{\beta} &= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}, \\
\hat{y} &= \mathbf{Q}\mathbf{Q}^T\mathbf{y},
\end{align}
$$

where $\mathbf{Q}$ is an $N\times\left(p+1\right)$ orthogonal matrix, $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$, and $\mathbf{R}$ is a $\left(p+1\right)\times\left(p+1\right)$ upper triangular matrix.

We make up a $N\times\left(N-p-1\right)$ orthogonal matrix $\mathbf{Q}_r$ satisfying that all columns of $\mathbf{Q}$ and $\mathbf{Q}_r$ are orthogonal to each other, in other words the $N \times N$ matrix $\mathbf{U}$ is a (real) unitary matrix:

$$
\begin{equation}
\label{eq:q-complemented-as-u}
\mathbf{U} = \left[ \begin{array}{cc} \mathbf{Q} & \mathbf{Q}_r \end{array} \right].
\end{equation}
$$

In the following derivations, we denote by $z_i = \left<u_i, \mathbf{\epsilon}\right>$ the inner product between the $i$th column of $\mathbf{U}$ and $\mathbf{\epsilon}$. First we look at $\hat{\beta}$:

$$
\begin{align}
\hat{\beta}
&= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y} \\
&= \mathbf{R}^{-1}\mathbf{Q}^T\left(\mathbf{X}\beta+\mathbf{\epsilon}\right) \\
&= \beta + \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{\epsilon}.
\end{align}
$$

We can see that $\hat{\beta}$ is a function of $z_1$, $z_2$, ..., and $z_{p+1}$.

What about $RSS\left(\hat{\beta}\right)$?

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}\mathbf{Q}^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p+1}^2\right) \\
&= z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2.
\end{align}
$$

Then $RSS\left(\hat{\beta}\right)$ is a function of $z_{p+2}$, $z_{p+3}$, ..., and $z_N$. Because $\epsilon_i$ are independent normal random variables and $\mathbf{U}$ is unitary, then $z_i$ are independent normal random variables, too. Finally we proved that $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are indenpendent of each other.

---

## Note [p:48 eq:3.13] f-statistic

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/26
Reference | [Wikipedia: F-distribution][wikipedia-f-distribution], [Wikipedia: F-test][wikipedia-f-test]

[wikipedia-f-distribution]: https://en.wikipedia.org/wiki/F-distribution
[wikipedia-f-test]: https://en.wikipedia.org/wiki/F-test

In (3.13) the $F$ statistic is given as

$$
\begin{equation}
\label{eq:f-statistic}
F = \frac{\left. \middle(RSS_0-RSS_1\middle) \middle/ \middle(p_1-p_0\middle) \right.}{\left. RSS_1 \middle/ \middle(N-p_1-1\middle) \right.},
\end{equation}
$$

where $RSS_1$ is the residual sum-of-squares for the least squares fit of the bigger model with $p_1+1$ parameters, and $RSS_0$ the same for the nested smaller model with $p_0+1$ parameters, having $p_1−p_0$ parameters constrained to be zero.

Here we are going to prove that $F$ statistic has a $F_{p_1−p_0,N−p_1−1}$ distribution, under the Gaussian assumptions, and the null hypothesis that the smaller model is correct.

Let's rearrange the cloumns in $\mathbf{X}$ so that the column vectors used by the larger model only appearing at the rightmost. Appling **QR** decomposition we get

$$
\begin{align}
\mathbf{X}
&= \mathbf{Q}\mathbf{R} \\
&= \left[ \begin{array}{cc} \mathbf{Q}_0 & \mathbf{Q}_s \end{array} \right] \left[ \begin{array}{cc} \mathbf{R}_0 & \mathbf{R}_{s1} \\ \mathbf{0} & \mathbf{R}_{s2} \end{array} \right], \\
\mathbf{X}_0 &= \mathbf{Q}_0\mathbf{R}_0,
\end{align}
$$

where $\mathbf{Q}_0$ is a $N \times \left(p_0+1\right)$ orthogonal matrix and $\mathbf{R}_0$
is a $(p_0+1) \times (p_0+1)$ upper triangular matrix. Then the RSSs can be simplified as

$$
\begin{align}
RSS_1
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}\mathbf{Q}^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p_1+1}^2\right) \\
&= z_{p_1+2}^2 + z_{p_1+3}^2 + \dots + z_N^2, \\
RSS_0
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH_0\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}_0\mathbf{Q}_0^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}_0^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}_0^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p_0+1}^2\right) \\
&= z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_N^2, \\
RSS_0 - RSS_1
&= \left(z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_N^2\right) - \left(z_{p_1+2}^2 + z_{p_1+3}^2 + \dots + z_N^2\right) \\
&= z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_{p_1+1}^2,
\end{align}
$$

where $\mathbf{U}$ is defined as $\eqref{eq:q-complemented-as-u}$.

Because $\epsilon_i$ are independent normal random variables and $\mathbf{U}$ is unitary, then $z_i$ are independent normal random variables, too. Then we have $\frac{RSS_1}{\sigma^2} \sim \chi_{N-p_1-1}^2$, $\frac{RSS_0-RSS_1}{\sigma^2} \sim \chi_{p_1-p_0}^2$, and those two are independent of each other. By the definition of [F-distribution][wikipedia-f-distribution], the $F$ statistic in $\eqref{eq:f-statistic}$ follows the $F$ distribution with $p_1-p_0$ and $N−p_1−1$ degrees of freedom respectively.

---

## Ex. 3.1 f-statistic-and-z-score

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/28
Reference | [Weatherwax and Epstein's solution][weatherwax-epstein-solution], [Wikipedia: Triangular matrix][wikipedia-triangular-matrix]

[weatherwax-epstein-solution]: http://waxworksmath.com/Authors/G_M/Hastie/WriteUp/weatherwax_epstein_hastie_solutions_manual.pdf
[wikipedia-triangular-matrix]: https://en.wikipedia.org/wiki/Triangular_matrix

**Ex. 3.1** Show that the $F$ statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding $z$-score (3.12).

**Proof**

Our proof is inspired by [Weatherwax and Epstein's solution][weatherwax-epstein-solution] whereas ours is more compact.

Rearrange the columns of $\mathbf{X}$ by putting $\mathbf{x_j}$ to the last position, under **QR** decomposition

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R}, \\
\hat{\beta} &= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}, \\
\hat{y} &= \mathbf{Q}\mathbf{Q}^T\mathbf{y},
\end{align}
$$

the error reduction by indroducing the new variable $X_j$ is

$$
\begin{equation}
RSS_0-RSS_1 = \left<q_j, \mathbf{y}\right>.
\end{equation}
$$

This fact is well explained in **Algorithm 3.1** on page 54.

$\mathbf{R}$ is an [upper triangular matrix][wikipedia-triangular-matrix], the inverse of it
$\mathbf{R}^{-1}$ is also an upper triangular matrix, and the diagonal entries of $\mathbf{R}^{-1}$ are reciprocal of the corresponding entries in $\mathbf{R}$. $\mathbf{R}^T$ and $\left(\mathbf{R}^T\right)^{-1}$ are hence lower triangular matrices.

It can be seen that

$$
\begin{equation}
\hat{\beta_j}^2 = \left<q_j,\mathbf{y}\right> \left/\right. r_{jj}^2,
\end{equation}
$$

where $r_{jj}$ is the $j$th (the last) diagonal entries of $\mathbf{R}$.

Because

$$
\begin{align}
\mathbf{X}^T\mathbf{X}
&= \mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{R} = \mathbf{R}^T\mathbf{R}, \\
\left(\mathbf{X}^T\mathbf{X}\right)^{-1}
&= \mathbf{R}^{-1}\left(\mathbf{R}^T\right)^{-1},
\end{align}
$$

$v_j$, the $j$th (the last) diagonal element of $\left(\mathbf{X}^T\mathbf{X}\right)^{-1}$, will be $1 \left/\right. r_{jj}^2$.

Consequently,

$$
\begin{align}
z_j^2
&= \left(\frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}}\right)^2
= \frac{\hat{\beta_j}^2}{\hat{\sigma}^2v_j}
= \frac{\left<q_j,\mathbf{y}\right> \left/\right. r_{jj}^2}{\hat{\sigma}^2\left(1 \left/\right. r_{jj}^2\right)}
= \frac{\left<q_j,\mathbf{y}\right>}{\hat{\sigma}^2}, \\
F
&= \frac{\left. \middle(RSS_0-RSS_1\middle) \middle/ \middle(p_1-p_0\middle) \right.}{\left. RSS_1 \middle/ \middle(N-p_1-1\middle) \right.}
= \frac{\left<q_j, \mathbf{y}\right>}{\hat{\sigma}^2};
\end{align}
$$

Q.E.D.

---

## Ex. 3.2 confidence-intervals

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/29

**Ex. 3.2** Given data on two variables $X$ and $Y$ , consider fitting a cubic polynomial regression model $f\left(X\right) = \sum_{j=0}^3\beta_jX^j$. In addition to plotting the fitted curve, you would like a 95% confidence band about the curve. Consider the following two approaches:

1. At each point $x_0$, form a 95% confidence interval for the linear function $a^T\beta = \sum_{j=0}^3\beta_jx_0^j$.
1. Form a 95% confidence set for $\beta$ as in (3.15), which in turn generates confidence intervals for $f\left(x_0\right)$.

How do these approaches differ? Which band is likely to be wider? Conduct a small simulation experiment to compare the two methods.

**Solution**

For the first case, $a^T\hat{\beta}$ follows a normal distribution $N\left(a^T\beta, a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a \cdot \sigma^2\right)$ (see $\eqref{eq:note-beta-hat-variance}$ for the variance of $\hat{\beta}$). The confidence interval of $a^T\beta$ is then given by

$$
\begin{equation}
C_{a^T\beta}^l = \left(a^T\hat{\beta}-1.96\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma},\; a^T\hat{\beta}+1.96\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma}\right)
\end{equation}
$$

approximately, where $1.96 = z^{\left(1-0.025\right)}$ is the $0.975$ percentile of the normal distribution (see also [Wikipedia: Standard normal table](https://en.wikipedia.org/wiki/Standard_normal_table)). We can do this approximation when the size of training data $N$ is big enough so that $t_{N-p-1}$ approaches $N\left(0, 1\right)$.

For the second case, we need to find the mininum and maximum of $a^T\beta$ when $\beta$ is in the confidence set given by (3.15)

$$
\begin{equation}
C_{\beta} = \left\{\beta\middle|\left(\hat{\beta}-\beta\right)^T\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) \le \hat{\sigma}^2{\chi_{p+1}^2}^{\left(1-\alpha\right)}\right\}.
\end{equation}
$$

In order to find the radius of the confidence interval under the constraints that $\beta$ is in the ellipse above, we need to find

$$
\begin{equation}
\label{eq:ex-3.2-beta-maximization}
\beta^r = \mathop{\mathrm{argmax}}_{\beta} \left\{a^T\left(\hat{\beta}-\beta\right) + \lambda\left[\left(\hat{\beta}-\beta\right)^T\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) - S\right]\right\},
\end{equation}
$$

where $S = 9.49\hat{\sigma}^2$ and $9.49 = {\chi_4^2}^{\left(1-0.95\right)}$ is the $0.95$ percentile of the chi-squared distribution (see also [Wikipedia: Chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution)).

Differentiating the Lagrangian form in the braces of $\eqref{eq:ex-3.2-beta-maximization}$ with respect to $\lambda$ and $\beta$ and set the derivatives to zero we obtain

$$
\begin{align}
\left(\hat{\beta}-\beta\right)^T\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) - S &= 0 \\
\label{eq:ex-3.2-partial-to-beta}
a + 2\lambda\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) &= 0.
\end{align}
$$

Continue on $\eqref{eq:ex-3.2-partial-to-beta}$,

$$
\begin{align}
\left(\hat{\beta}-\beta\right)^Ta + 2\lambda\left(\hat{\beta}-\beta\right)^T\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) = 0 \\
2\lambda = -\frac{\left(\hat{\beta}-\beta\right)^Ta}{S} \\
a - \frac{\left(\hat{\beta}-\beta\right)^Ta}{S} \mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) = 0 \\
\left(\hat{\beta}-\beta\right)^Ta \left(\hat{\beta}-\beta\right) = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}aS \\
\left[\left(\hat{\beta}-\beta\right)^Ta\right] \left[a^T\left(\hat{\beta}-\beta\right)\right] = a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}aS \\
a^T\left(\hat{\beta}-\beta\right) = \sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}aS},
\end{align}
$$

we reach

$$
\begin{equation}
a^T\left(\hat{\beta}-\beta^r\right) = \sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}aS} = 3.08\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma}.
\end{equation}
$$

The confidence interval of $a^T\beta$ is then given by

$$
\begin{equation}
C_{a^T\beta}^{\beta-\mathrm{set}} = \left(a^T\hat{\beta}-3.08\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma},\; a^T\hat{\beta}+3.08\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma}\right).
\end{equation}
$$

We can see that the band of $C_{a^T\beta}^{\beta-\mathrm{set}}$ is wider than that of $C_{a^T\beta}^l$, it is because that some $\beta$ not in $C_{\beta}$ can still make $a^T\beta$ close to $a^T\hat{\beta}$ enough. Therefore if we are more interested in the confidence of the prediction rather than the confidence of the parameter estimation, we should use $C_{a^T\beta}^l$.

**Code**

Here we give our code of confidence interval calculation and plotting.

```{r, fig.width = 12, fig.height = 9}
set.seed(32)

# ------------------------------------------------------------
# training data generation
# ------------------------------------------------------------

training.size <- 100
df <- training.size - 4

CalcFeatures <- function(x1) {
  x2 <- x1 ^ 2
  x3 <- x1 ^ 3
  cbind(x0=rep(1, length(x1)), x1=x1, x2=x2, x3=x3)
}

error <- rnorm(training.size, sd=2)
beta <- c(rnorm(1), rnorm(1, mean=2), rnorm(1, mean=1), rnorm(1, mean=0))

x <- CalcFeatures(rnorm(training.size))
y <- x %*% beta + error
train.data <- data.frame(x)
train.data$y = y

plot(train.data$x1, train.data$y, main='Confidence Intervals of Cubic of X',
     xlab='input', ylab='response', lwd=2, cex=1.5)

# ------------------------------------------------------------
# model fitting
# ------------------------------------------------------------

lm.fit <- lm(y ~ x1+x2+x3, data=train.data)
beta.hat <- coef(lm.fit)

x.range <- seq(min(train.data$x1), max(train.data$x1), length.out=100)
test.x <- CalcFeatures(x.range)
prediction.y <- predict(lm.fit, as.data.frame(test.x), interval='confidence')
fitted.values <- as.data.frame(prediction.y)

# ------------------------------------------------------------
# confidence bands calculation
# ------------------------------------------------------------

test.data <- data.frame(test.x)

sigma.hat <- sqrt(sum(lm.fit$residuals^2) / df)
sample.cov <- t(x) %*% x
inv.sample.cov <- solve(sample.cov)
t.quantile <- qt(0.975, df)
chi.quantile <- qchisq(0.95, 4)

bands <- apply(test.data, 1, function(x) {
  fitted <- x %*% beta.hat
  factor <- sqrt(t(x) %*% inv.sample.cov %*% x) * sigma.hat
  linear.band <- factor * t.quantile
  beta.set.band <- factor * sqrt(chi.quantile)
  c(fitted=fitted, linear.band=linear.band, beta.set.band=beta.set.band)
})
bands <- as.data.frame(t(bands))

AssertEqual <- function(expected, actual) {
  stopifnot(abs(expected - actual) < 1e-7)
}

AssertEqual(fitted.values$fit, bands$fitted)
AssertEqual(fitted.values$upr - fitted.values$fit, bands$linear.band)

# ------------------------------------------------------------
# boundary beta curves
# ------------------------------------------------------------

svd.decomposed <- svd(sample.cov)
d.sqrt <- matrix(0, ncol=4, nrow=4)
for (i in 1:4) {
  d.sqrt[i, i] <- sqrt(svd.decomposed$d[i])
}
a.matrix <- d.sqrt %*% t(svd.decomposed$v)
inv.a.matrix <- solve(a.matrix)
length.bound <- sigma.hat * sqrt(chi.quantile)

SampleBoundaryBeta <- function() {
  direction <- rnorm(4)
  circle.point <- direction / sqrt(sum(direction^2)) * length.bound
  inv.a.matrix %*% circle.point + beta.hat
}

DrawBoundaryBeta <- function(beta) {
  lines(test.data$x1, test.x %*% beta,
        col='darkgray', lty='dotted', lwd=1)
}

for (i in 1:100) {
  beta.sampled <- SampleBoundaryBeta()
  DrawBoundaryBeta(beta.sampled)
}

# ------------------------------------------------------------
# draw other more important curves
# ------------------------------------------------------------

lines(test.data$x1, bands$fitted - bands$beta.set.band,
      col='blue', lty='longdash', lwd=2)
lines(test.data$x1, bands$fitted + bands$beta.set.band,
      col='blue', lty='longdash', lwd=2)
lines(test.data$x1, fitted.values$lwr,
      col='purple', lty='longdash', lwd=2)
lines(test.data$x1, fitted.values$upr,
      col='purple', lty='longdash', lwd=2)
lines(test.data$x1, fitted.values$fit,
      col='green', lty='solid', lwd=2)
lines(test.data$x1, test.x %*% beta,
      col='red', lty='solid', lwd=2)

```

In the figure above, the plots are the training data points, the red solid line the true model curve (excluding noise), the green solid line the linear regression fit.

The purple dashed lines denote the lower and upper confidence intervals which are got from the ```lm``` fit. That confidence interval is also calucated in our code and verified against what we got from the ```lm``` fit. Note that we use the percentile of Student's $t$-distribution instead of the percentile of the standard normal distribution in order to accurately match the ```lm``` fit result.

The blue dashed lines denote the lower and upper confidence intervals based on the confidence set of $\beta$. It is wider than the normal confidence interval. The dark gray dotted lines are got by using the $\beta$ at the boundary of the confidence set of $\beta$. It can been see that they are well bounded by the blue confidence bands but are not bounded by the purple one.

The sampling of the boundary $\beta$ is a little bit tricky. We use the SVD of $\mathbf{X}^T\mathbf{X}$ to achieve that.

---

## Ex. 3.4 least-squares-by-qr

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/1

**Ex. 3.4** Show how the vector of least squares coefficients can be obtained from a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Represent your solution in terms of the QR decomposition of $\mathbf{X}$.

**Solution**

In the beginning, we initalize $\mathbf{z}_0 = \mathbf{x}_0 = \mathbf{1}$. Regressing $\mathbf{x}_1$ on $\mathbf{z}_0$ we get the coefficient $\hat{\gamma}_{01} = \left<\mathbf{z}_0, \mathbf{x}_j\right>\left/\right.\left<\mathbf{z}_0, \mathbf{z}_0\right>$ and the residual $\mathbf{z}_1 = \mathbf{x}_1 - \hat{\gamma}_{01}\mathbf{z}_0$ and hence $\mathbf{x}_1 = \hat{\gamma}_{01}\mathbf{z}_0 + \mathbf{z}_1$. Continue on this procedure, we can get $\mathbf{x}_2 = \hat{\gamma}_{02}\mathbf{z}_0 + \hat{\gamma}_{12}\mathbf{z}_1 + \mathbf{z}_2$, $\mathbf{x}_3 = \hat{\gamma}_{03}\mathbf{z}_0 + \hat{\gamma}_{13}\mathbf{z}_1 + \hat{\gamma}_{23}\mathbf{z}_2 + \mathbf{z}_3$, ..., and $\mathbf{x}_p = \hat{\gamma}_{0p}\mathbf{z}_0 + \hat{\gamma}_{1p}\mathbf{z}_1 + \dots + \hat{\gamma}_{\left(p-1\right)p}\mathbf{z}_{p-1} + \mathbf{z}_p$.

Rewriting equations above with matrix operations we get

$$
\begin{equation}
\mathbf{X} = \mathbf{Z}\mathbf{\Gamma},
\end{equation}
$$

where $\mathbf{z}_j$ are orthogonal of each other and $\mathbf{\Gamma}$ is an upper triangular matrix with all diagonal entries be 1. It is convenient to make length of the orthogonal vectors be 1, so we do it as

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{\Gamma} \\
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R},
\end{align}
$$

where $\mathbf{D}$ is a diagonal matrix with $j$th diagonal entry $D_{jj} = \left\Vert \mathbf{z}_j \right\Vert$. This way, we make $\mathbf{Q}$ an orthogonal matrix. $\mathbf{R}$ is still an upper triangular matrix but its diagonal entry are now $r_{jj} = \left\Vert \mathbf{z}_j \right\Vert$.

The vector of least square coefficients can be represented by $\mathbf{Q}$ and $\mathbf{R}$:

$$
\begin{align}
\hat{\beta}
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y} \\
&= \left(\mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{R}\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y} \\
&= \left(\mathbf{R}^T\mathbf{R}\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y} \\
&= \mathbf{R}^{-1}\left(\mathbf{R}^T\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y} \\
&= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}.
\end{align}
$$

$\mathbf{R}^{-1}$ is also an upper triangular matrix and its entries can be sovled by

$$
\begin{align}
r_{jj}^{-} &= \frac{1}{r_{jj}} \\
r_{kj}^{-} &= -\frac{\sum_{l=k+1}^j r_{kl}r_{lj}^{-}}{r_{kk}},\;k=j-1,\dots,0;
\end{align}
$$

starting from column $0$ to column $p$ and for column $j$ starting from $r_{jj}^{-}$ to $r_{0j}^{-}$.

---

## Ex. 3.5 ridge-inputs-centered

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/2

**Ex. 3.5** Consider the ridge regression problem (3.41). Show that this problem is equivalent to the problem

$$
\begin{equation}
\hat{\beta}^c = \mathop{\mathrm{argmin}}_{\beta^c}\left\{\sum_{i=1}^N\left[y_i-\beta_0^c-\sum_{j=1}^p\left(x_{ij}-\bar{x}_j\right)\beta_j^c\right]^2 + \lambda\sum_{j=1}^p{\beta_j^c}^2\right\}.
\end{equation}
$$

Give the correspondence between $\beta^c$ and the original $\beta$ in (3.41). Characterize the solution to this modified criterion. Show that a similar result holds for the lasso.

**Solution**

Rewriting the optimizating criterion

$$
\begin{align}
RSS^c
&= \sum_{i=1}^N\left[y_i-\beta_0^c-\sum_{j=1}^p\left(x_{ij}-\bar{x}_j\right)\beta_j^c\right]^2 + \lambda\sum_{j=1}^p{\beta_j^c}^2 \\
&= \sum_{i=1}^N\left[y_i-\left(\beta_0^c-\sum_{j=1}^p\bar{x}_j\beta_j^c\right)-\sum_{j=1}^px_{ij}\beta_j^c\right]^2 + \lambda\sum_{j=1}^p{\beta_j^c}^2,
\end{align}
$$

we can see that the correspondence between $\beta^c$ and the original $\beta$ in (3.41) is

$$
\begin{equation}
\left\{
\begin{aligned}
\beta_j &= \beta_j^c, & 1 \le j \le p \\
\beta_0 &= \beta_0^c-\sum_{j=1}^p\bar{x}_j\beta_j^c,
\end{aligned}
\right.
\end{equation}
$$

or equivalently

$$
\begin{equation}
\left\{
\begin{aligned}
\beta_j^c &= \beta_j, & 1 \le j \le p \\
\beta_0^c &= \beta_0+\sum_{j=1}^p\bar{x}_j\beta_j.
\end{aligned}
\right.
\end{equation}
$$

We now proceed to show that $\beta_0^c=\bar{y}=\frac{1}{N}\sum_1^Ny_i$ as stated on page 64. Differentiating $RSS^c$ with respect to $\beta_0^c$ and setting the derivative to zero,

$$
\begin{align}
0
&= \frac{\partial{RSS^c}}{\partial{\beta_0^c}} \\
&= 2\sum_{i=1}^N\left[\beta_0^c-y_i+\sum_{j=1}^p\left(x_{ij}-\bar{x}_j\right)\beta_j^c\right] \\
&= 2N\beta_0^c - 2\sum_{i=1}^Ny_i + 2\sum_{i=1}^N\sum_{j=1}^p\left(x_{ij}-\bar{x}_j\right)\beta_j^c \\
&= 2N\beta_0^c - 2\sum_{i=1}^Ny_i + 2\sum_{j=1}^p\beta_j^c\sum_{i=1}^N\left(x_{ij}-\bar{x}_j\right) \\
&= 2N\beta_0^c - 2\sum_{i=1}^Ny_i + 2\sum_{j=1}^p\beta_j^c\times0 \\
&= 2N\beta_0^c - 2\sum_{i=1}^Ny_i,
\end{align}
$$

we then reach the conclusion we want to draw: $\beta_0^c=\bar{y}=\frac{1}{N}\sum_1^Ny_i$.

Apparently, a similar result holds for the lasso.

---

## Ex. 3.6 ridge-as-posterior-mean

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/2
Reference | [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution]

[wikipedia-multivariate-normal-distribution]: https://en.wikipedia.org/wiki/Multivariate_normal_distribution

**Ex. 3.6** Show that the ridge regression estimate is the mean (and mode) of the posterior distribution, under a Gaussian prior $\beta \sim N\left(0, \tau^2I\right)$, and Gaussian sampling model $\mathbf{y} \sim N\left(\mathbf{X}β, \sigma^2I\right)$. Find the relationship between the regularization parameter $\lambda$ in the ridge formula, and the variances $\tau^2$ and $\sigma^2$.

**Proof**

First we calculate the log likelihood of the training data:

$$
\begin{align}
\mathcal{L}\left(\beta\right)
&= \left(2\pi\right)^{-\frac{N}{2}}\left|\sigma^2I\right|^{-\frac{1}{2}} \mathrm{exp}\left(-\frac{1}{2}\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\sigma^2I\right)^{-1}\left(\mathbf{y}-\mathbf{X}\beta\right)\right) \\
&= \left(2\pi\right)^{-\frac{N}{2}}\sigma^{-N} \mathrm{exp}\left(-\frac{1}{2\sigma^2}\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right)\right), \\
-\log\mathcal{L}\left(\beta\right)
&= \frac{1}{2\sigma^2}\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + C_1.
\end{align}
$$

The minus log prior of $\beta$ can also be calculated as:

$$
\begin{align}
p\left(\beta\right)
&= \left(2\pi\right)^{-\frac{p}{2}}\left|\tau^2I\right|^{-\frac{1}{2}} \mathrm{exp}\left(-\frac{1}{2}\beta^T\left(\tau^2I\right)^{-1}\beta\right) \\
&= \left(2\pi\right)^{-\frac{p}{2}}\tau^{-p} \mathrm{exp}\left(-\frac{1}{2\tau^2}\beta^T\beta\right), \\
-\log p\left(\beta\right)
&= \frac{1}{2\tau^2}\beta^T\beta + C_2.
\end{align}
$$

The posterior density function is proportional to the likelihood of the training data multiplying the prior density function,

$$
\begin{equation}
p\left(\beta|\mathbf{y}\right) \propto \mathcal{L}\left(\beta\right) \times p\left(\beta\right),
\end{equation}
$$

hence the minus log posterior density function will be

$$
\begin{align}
-\log p\left(\beta|\mathbf{y}\right)
&= -\log\mathcal{L}\left(\beta\right) -\log p\left(\beta\right) + C_3 \\
&= \frac{1}{2\sigma^2}\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \frac{1}{2\tau^2}\beta^T\beta + C_1 + C_2 + C_3 \\
&= \frac{1}{2\sigma^2}\left[\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \frac{\sigma^2}{\tau^2}\beta^T\beta\right] + C_4 \\
&= \frac{1}{2\sigma^2}\left[\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \lambda\beta^T\beta\right] + C_4.
\end{align}
$$

Therefore, minimizing the ridge loss function in (3.43) is equivalent to maximizing the log posterior and the relationship between $\lambda$, $\sigma^2$ and $\tau^2$ is $\lambda=\sigma^2\left/\right.\tau^2$.

The mode (maximun) of the posterior distribution is also the mean, because the log likelihood is a quadratic function of $\beta$.
