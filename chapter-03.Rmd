---
title: "Linear Methods for Regression"
output:
  html_document:
    toc: yes
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

---

## Note [p:47 eq:3.8 eq:3.10] beta-hat-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution]

[wikipedia-multivariate-normal-distribution]: https://en.wikipedia.org/wiki/Multivariate_normal_distribution

The least squares parameter estimates $\hat{\beta}$ is given by equation (3.6)

$$
\begin{equation}
\hat{\beta} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}.
\end{equation}
$$

Under the assumption that the observations $y_i$ are uncorrelated and have constant variance $\sigma^2$, and that the $x_i$ are fixed (non random), we now have

$$
\begin{equation}
E\left(\hat{\beta}\right) = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right),
\end{equation}
$$

and the variance–covariance matrix of the $\hat{\beta}$ becomes

$$
\begin{align}
Var\left(\hat{\beta}\right)
&= E\left\{ \left[\hat{\beta} - E\left(\hat{\beta}\right)\right] \left[\hat{\beta} - E\left(\hat{\beta}\right)\right]^T \right\} \\
&= E\left\{ \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right] \left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right)\right]^T \right\} \\
&= E\left[ \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \right] \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left[\left(\mathbf{y}-E\left(\mathbf{y}\right)\right) \left(\mathbf{y}-E\left(\mathbf{y}\right)\right)^T\right] \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \sigma^2I \mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X} \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \sigma^2 \\
\label{eq:note-beta-hat-variance}
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2.
\end{align}
$$

Further, we assume that (3.1) is the correct model for the mean; that is, the conditional expectation of $Y$ is linear in $X_1$, ..., $X_p$, and the deviations of $Y$ around its expectation are additive and Gaussian, i.e.,

$$
\begin{equation}
Y = X^T\beta + \epsilon,
\end{equation}
$$

where the error $\epsilon$ is a Gaussian random variable with expectation zero and variance $\sigma^2$, written $\epsilon \sim N\left(0, \sigma^2\right)$. According to [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution], any linear transformation of independent standard normal random variables is a multivariate normal distribution, hence $\hat{\beta}$ is a multivariate normal distribution. The expectation of $\hat{\beta}$ can be derived straightforward as

$$
\begin{align}
E\left(\hat{\beta}\right)
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T E\left(\mathbf{y}\right) \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X}\beta \\
&= \beta.
\end{align}
$$

Finally we proved that:

$$
\begin{equation}
\label{eq:beta-hat-distribution}
\hat{\beta} \sim N\left(\beta, \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\sigma^2\right).
\end{equation}
$$

---

## Note [p:47 eq:3.11] training-rss-distribution

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/23
Reference | [Wikipedia: Idempotent matrix][wikipedia-idempotent-matrix], [Wikipedia: Positive-definite matrix][wikipedia-positive-definite-matrix], [Wikipedia: Spectral theorem][wikipedia-spectral-theorem], [Wikipedia: Chi-squared distribution][wikipedia-chi-squared-distribution]

[wikipedia-idempotent-matrix]: https://en.wikipedia.org/wiki/Idempotent_matrix
[wikipedia-positive-definite-matrix]: https://en.wikipedia.org/wiki/Positive-definite_matrix
[wikipedia-spectral-theorem]: https://en.wikipedia.org/wiki/Spectral_theorem
[wikipedia-chi-squared-distribution]: https://en.wikipedia.org/wiki/Chi-squared_distribution

In the solution to **Ex. 2.9**, we got the representation of training RSS by the hat matrix

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \left(\mathbf{y} - \hat{\mathbf{y}}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\hat{\beta}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \left(\mathbf{X}\beta + \mathbf{\epsilon}\right)\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{X}\beta + \mathbf{\epsilon} - \mathbf{X}\beta - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{y} - \hat{\mathbf{y}}\right) \\
&= \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right)^T \left(\mathbf{\epsilon} - H\mathbf{\epsilon}\right) \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - 2\mathbf{\epsilon}^TH\mathbf{\epsilon} + \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon},
\end{align}
$$

where $H$ is:

1. symmetric: $H^T = H$;
1. [idempotent][wikipedia-idempotent-matrix]: $HH = H$; and
1. [positive semi-definite][wikipedia-positive-definite-matrix]: $z^THz = z^TH^THz = \left(Hz\right)^T\left(Hz\right) \ge 0$.

[Eigenvalues of idempotent matrices are either 0 or 1](http://everything2.com/title/Eigenvalues+of+idempotent+matrices+are+either+0+or+1): For any idempotent matrix $A$,

$$
\begin{equation}
\lambda v = Av = A^2v = A\left(Av\right) = A\lambda v = \lambda Av = \lambda^2 v.
\end{equation}
$$

$H$ is also a normal matrix ($H^{\ast}H = HH^{\ast}$), then it can be unitary diagonalized according to the [Spectral theorem][wikipedia-spectral-theorem]:

$$
\begin{equation}
H = UDU^{\ast},
\end{equation}
$$

where $U$ is a real unitary matrix and $U^{\ast} = U^T = U^{-1}$, and $D$ is a diagonal matrix with eigenvalues appearing at the diagonal entries. Under appropriate permutations, we can rearrange the eigenvalues of $1$s before all the $0$s.

Then

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^TUU^T\mathbf{\epsilon} - \mathbf{\epsilon}^TUDU^T\mathbf{\epsilon} \\
&= \left(U^T\mathbf{\epsilon}\right)^T\left(U^T\mathbf{\epsilon}\right) - \left(U^T\mathbf{\epsilon}\right)^TD\left(U^T\mathbf{\epsilon}\right) \\
&= \mathbf{z}^T\mathbf{z} - \mathbf{z}^TD\mathbf{z} \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p+1}^2\right) \\
&= z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2,
\end{align}
$$

where $z_i = \left<u_i, \mathbf{\epsilon}\right>$ is the inner product between the $i$th column of $U$ and $\mathbf{\epsilon}$. It is easy to show that all the $z_i$ follow Gaussian distributions with variance $\sigma^2$ (norms of $u_i$ are 1) and are independent of each other (columns of $U$ are orthogonal to each other), in other words, $RSS\left(\hat{\beta}\right)$ is a sum of squared $N-p-1$ independent normal variables. By definition, $\left. RSS\middle(\hat{\beta}\middle)\middle/\sigma^2 \right.$ follows a [Chi-squared distribution][wikipedia-chi-squared-distribution]:

$$
\begin{equation}
\label{eq:rss-distribution}
\frac{RSS\left(\hat{\beta}\right)}{\sigma^2} \sim \chi_{N-p-1}^2,
\end{equation}
$$

or in another form

$$
\begin{equation}
\left(N-p-1\right)\hat{\sigma}^2 \sim \sigma^2\chi_{N-p-1}^2.
\end{equation}
$$

---

## Note [p:48 eq:3.12] t-statistic

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/26
Reference | [Wikipedia: Student's t-distribution][wikipedia-student's-t-distribution], [Wikipedia: Student's t-test][wikipedia-student's-t-test]

[wikipedia-student's-t-distribution]: https://en.wikipedia.org/wiki/Student's_t-distribution
[wikipedia-student's-t-test]: https://en.wikipedia.org/wiki/Student's_t-test

Here we are going to prove that _Z-score_

$$
\begin{equation}
z_j = \frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}},
\end{equation}
$$

where $v_j$ is the $j$th diagonal element of $\left(\mathbf{X}^T\mathbf{X}\right)^{−1}$, follows a Student's $t$-distribution under the null hypothesis that $\beta_j = 0$.

Recall the distrubution of $\hat{\beta}$ in $\eqref{eq:beta-hat-distribution}$, it is easy to show that $\frac{\hat{\beta_j}-0}{\sigma\sqrt{v_j}}$ follows the standard normal distribution. And as shown in $\eqref{eq:rss-distribution}$, $\frac{RSS\left(\hat{\beta}\right)}{\sigma^2}$ follows the chi-squared distribution with $N-p-1$ degrees of freedom, therefore if we can proof that $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are independent of each other then by the definition of [Student's t-distribution][wikipedia-student's-t-distribution],

$$
\begin{equation}
z_j = \frac{\frac{\hat{\beta_j}-0}{\sigma\sqrt{v_j}}}{\sqrt{\frac{RSS\left(\hat{\beta}\right)}{\sigma^2 \left(N-p-1\right)}}} = \frac{\hat{\beta_j}}{\sqrt{\frac{RSS\left(\hat{\beta}\right)}{N-p-1}}\sqrt{v_j}} = \frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}}
\end{equation}
$$

follows the $t$ distribution with $N−p−1$ degrees of freedom.

We now proceed to proof $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are independent of each other by making use of **QR** decomposition of $\mathbf{X}$ as shown in (3.31), (3.31) and (3.32)

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R}, \\
\hat{\beta} &= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}, \\
\hat{y} &= \mathbf{Q}\mathbf{Q}^T\mathbf{y},
\end{align}
$$

where $\mathbf{Q}$ is an $N\times\left(p+1\right)$ orthogonal matrix, $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$, and $\mathbf{R}$ is a $\left(p+1\right)\times\left(p+1\right)$ upper triangular matrix.

We make up a $N\times\left(N-p-1\right)$ orthogonal matrix $\mathbf{Q}_r$ satisfying that all columns of $\mathbf{Q}$ and $\mathbf{Q}_r$ are orthogonal to each other, in other words the $N \times N$ matrix $\mathbf{U}$ is a (real) unitary matrix:

$$
\begin{equation}
\label{eq:q-complemented-as-u}
\mathbf{U} = \left[\begin{array}{cc} \mathbf{Q} & \mathbf{Q}_r \end{array}\right].
\end{equation}
$$

In the following derivations, we denote by $z_i = \left<u_i, \mathbf{\epsilon}\right>$ the inner product between the $i$th column of $\mathbf{U}$ and $\mathbf{\epsilon}$. First we look at $\hat{\beta}$:

$$
\begin{align}
\hat{\beta}
&= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y} \\
&= \mathbf{R}^{-1}\mathbf{Q}^T\left(\mathbf{X}\beta+\mathbf{\epsilon}\right) \\
&= \beta + \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{\epsilon}.
\end{align}
$$

We can see that $\hat{\beta}$ is a function of $z_1$, $z_2$, ..., and $z_{p+1}$.

What about $RSS\left(\hat{\beta}\right)$?

$$
\begin{align}
RSS\left(\hat{\beta}\right)
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}\mathbf{Q}^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_p^2 + z_{p+1}^2 + z_{p+2}^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p+1}^2\right) \\
&= z_{p+2}^2 + z_{p+3}^2 + \dots + z_N^2.
\end{align}
$$

Then $RSS\left(\hat{\beta}\right)$ is a function of $z_{p+2}$, $z_{p+3}$, ..., and $z_N$. Because $\epsilon_i$ are independent normal random variables and $\mathbf{U}$ is unitary, then $z_i$ are independent normal random variables, too. Finally we proved that $\hat{\beta}$ and $RSS\left(\hat{\beta}\right)$ are indenpendent of each other.

---

## Note [p:48 eq:3.13] f-statistic

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/26
Reference | [Wikipedia: F-distribution][wikipedia-f-distribution], [Wikipedia: F-test][wikipedia-f-test]

[wikipedia-f-distribution]: https://en.wikipedia.org/wiki/F-distribution
[wikipedia-f-test]: https://en.wikipedia.org/wiki/F-test

In (3.13) the $F$ statistic is given as

$$
\begin{equation}
\label{eq:f-statistic}
F = \frac{\left. \middle(RSS_0-RSS_1\middle) \middle/ \middle(p_1-p_0\middle) \right.}{\left. RSS_1 \middle/ \middle(N-p_1-1\middle) \right.},
\end{equation}
$$

where $RSS_1$ is the residual sum-of-squares for the least squares fit of the bigger model with $p_1+1$ parameters, and $RSS_0$ the same for the nested smaller model with $p_0+1$ parameters, having $p_1−p_0$ parameters constrained to be zero.

Here we are going to prove that $F$ statistic has a $F_{p_1−p_0,N−p_1−1}$ distribution, under the Gaussian assumptions, and the null hypothesis that the smaller model is correct.

Let's rearrange the cloumns in $\mathbf{X}$ so that the column vectors used by the larger model only appearing at the rightmost. Appling **QR** decomposition we get

$$
\begin{align}
\mathbf{X}
&= \mathbf{Q}\mathbf{R} \\
&= \left[\begin{array}{cc} \mathbf{Q}_0 & \mathbf{Q}_s \end{array}\right] \left[\begin{array}{cc} \mathbf{R}_0 & \mathbf{R}_{s1} \\ \mathbf{0} & \mathbf{R}_{s2} \end{array}\right], \\
\mathbf{X}_0 &= \mathbf{Q}_0\mathbf{R}_0,
\end{align}
$$

where $\mathbf{Q}_0$ is a $N \times \left(p_0+1\right)$ orthogonal matrix and $\mathbf{R}_0$
is a $(p_0+1) \times (p_0+1)$ upper triangular matrix. Then the RSSs can be simplified as

$$
\begin{align}
RSS_1
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}\mathbf{Q}^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p_1+1}^2\right) \\
&= z_{p_1+2}^2 + z_{p_1+3}^2 + \dots + z_N^2, \\
RSS_0
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^TH_0\mathbf{\epsilon} \\
&= \mathbf{\epsilon}^T\mathbf{\epsilon} - \mathbf{\epsilon}^T\mathbf{Q}_0\mathbf{Q}_0^T\mathbf{\epsilon} \\
&= \left(\mathbf{U}^T\mathbf{\epsilon}\right)^T\left(\mathbf{U}^T\mathbf{\epsilon}\right) - \left(\mathbf{Q}_0^T\mathbf{\epsilon}\right)^T\left(\mathbf{Q}_0^T\mathbf{\epsilon}\right) \\
&= \left(z_1^2 + z_2^2 + \dots + z_N^2\right) - \left(z_1^2 + z_2^2 + \dots + z_{p_0+1}^2\right) \\
&= z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_N^2, \\
RSS_0 - RSS_1
&= \left(z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_N^2\right) - \left(z_{p_1+2}^2 + z_{p_1+3}^2 + \dots + z_N^2\right) \\
&= z_{p_0+2}^2 + z_{p_0+3}^2 + \dots + z_{p_1+1}^2,
\end{align}
$$

where $\mathbf{U}$ is defined as $\eqref{eq:q-complemented-as-u}$.

Because $\epsilon_i$ are independent normal random variables and $\mathbf{U}$ is unitary, then $z_i$ are independent normal random variables, too. Then we have $\frac{RSS_1}{\sigma^2} \sim \chi_{N-p_1-1}^2$, $\frac{RSS_0-RSS_1}{\sigma^2} \sim \chi_{p_1-p_0}^2$, and those two are independent of each other. By the definition of [F-distribution][wikipedia-f-distribution], the $F$ statistic in $\eqref{eq:f-statistic}$ follows the $F$ distribution with $p_1-p_0$ and $N−p_1−1$ degrees of freedom respectively.

---

## Ex. 3.1 f-statistic-and-z-score

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/28
Reference | [Weatherwax and Epstein's solution][weatherwax-epstein-solution], [Wikipedia: Triangular matrix][wikipedia-triangular-matrix]

[weatherwax-epstein-solution]: http://waxworksmath.com/Authors/G_M/Hastie/WriteUp/weatherwax_epstein_hastie_solutions_manual.pdf
[wikipedia-triangular-matrix]: https://en.wikipedia.org/wiki/Triangular_matrix

**Ex. 3.1** Show that the $F$ statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding $z$-score (3.12).

**Proof**

Our proof is inspired by [Weatherwax and Epstein's solution][weatherwax-epstein-solution] whereas ours is more compact.

Rearrange the columns of $\mathbf{X}$ by putting $\mathbf{x_j}$ to the last position, under **QR** decomposition

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R}, \\
\hat{\beta} &= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}, \\
\hat{y} &= \mathbf{Q}\mathbf{Q}^T\mathbf{y},
\end{align}
$$

the error reduction by indroducing the new variable $X_j$ is

$$
\begin{equation}
RSS_0-RSS_1 = \left<q_j, \mathbf{y}\right>^2.
\end{equation}
$$

This fact is well explained in **Algorithm 3.1** on page 54.

$\mathbf{R}$ is an [upper triangular matrix][wikipedia-triangular-matrix], the inverse of it
$\mathbf{R}^{-1}$ is also an upper triangular matrix, and the diagonal entries of $\mathbf{R}^{-1}$ are reciprocal of the corresponding entries in $\mathbf{R}$. $\mathbf{R}^T$ and $\left(\mathbf{R}^T\right)^{-1}$ are hence lower triangular matrices.

It can be seen that

$$
\begin{equation}
\hat{\beta_j}^2 = \left<q_j,\mathbf{y}\right>^2 \left/\right. r_{jj}^2,
\end{equation}
$$

where $r_{jj}$ is the $j$th (the last) diagonal entries of $\mathbf{R}$.

Because

$$
\begin{align}
\mathbf{X}^T\mathbf{X}
&= \mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{R} = \mathbf{R}^T\mathbf{R}, \\
\left(\mathbf{X}^T\mathbf{X}\right)^{-1}
&= \mathbf{R}^{-1}\left(\mathbf{R}^T\right)^{-1},
\end{align}
$$

$v_j$, the $j$th (the last) diagonal element of $\left(\mathbf{X}^T\mathbf{X}\right)^{-1}$, will be $1 \left/\right. r_{jj}^2$.

Consequently,

$$
\begin{align}
z_j^2
&= \left(\frac{\hat{\beta_j}}{\hat{\sigma}\sqrt{v_j}}\right)^2
= \frac{\hat{\beta_j}^2}{\hat{\sigma}^2v_j}
= \frac{\left<q_j,\mathbf{y}\right>^2 \left/\right. r_{jj}^2}{\hat{\sigma}^2\left(1 \left/\right. r_{jj}^2\right)}
= \frac{\left<q_j,\mathbf{y}\right>^2}{\hat{\sigma}^2}, \\
F
&= \frac{\left. \middle(RSS_0-RSS_1\middle) \middle/ \middle(p_1-p_0\middle) \right.}{\left. RSS_1 \middle/ \middle(N-p_1-1\middle) \right.}
= \frac{\left<q_j, \mathbf{y}\right>^2}{\hat{\sigma}^2};
\end{align}
$$

Q.E.D.

---

## Ex. 3.2 confidence-intervals

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/10/29

**Ex. 3.2** Given data on two variables $X$ and $Y$ , consider fitting a cubic polynomial regression model $f\left(X\right) = \sum_{j=0}^3\beta_jX^j$. In addition to plotting the fitted curve, you would like a 95% confidence band about the curve. Consider the following two approaches:

1. At each point $x_0$, form a 95% confidence interval for the linear function $a^T\beta = \sum_{j=0}^3\beta_jx_0^j$.
1. Form a 95% confidence set for $\beta$ as in (3.15), which in turn generates confidence intervals for $f\left(x_0\right)$.

How do these approaches differ? Which band is likely to be wider? Conduct a small simulation experiment to compare the two methods.

**Solution**

For the first case, $a^T\hat{\beta}$ follows a normal distribution $N\left(a^T\beta, a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a \cdot \sigma^2\right)$ (see $\eqref{eq:note-beta-hat-variance}$ for the variance of $\hat{\beta}$). The confidence interval of $a^T\beta$ is then given by

$$
\begin{equation}
C_{a^T\beta}^l = \left(a^T\hat{\beta}-1.96\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma},\; a^T\hat{\beta}+1.96\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma}\right)
\end{equation}
$$

approximately, where $1.96 = z^{\left(1-0.025\right)}$ is the $0.975$ percentile of the normal distribution (see also [Wikipedia: Standard normal table](https://en.wikipedia.org/wiki/Standard_normal_table)). We can do this approximation when the size of training data $N$ is big enough so that $t_{N-p-1}$ approaches $N\left(0, 1\right)$.

For the second case, we need to find the mininum and maximum of $a^T\beta$ when $\beta$ is in the confidence set given by (3.15)

$$
\begin{equation}
C_{\beta} = \left\{\beta\middle|\left(\hat{\beta}-\beta\right)^T\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) \le \hat{\sigma}^2{\chi_{p+1}^2}^{\left(1-\alpha\right)}\right\}.
\end{equation}
$$

In order to find the radius of the confidence interval under the constraints that $\beta$ is in the ellipse above, we need to find

$$
\begin{equation}
\label{eq:ex-3.2-beta-maximization}
\beta^r = \mathop{\mathrm{argmax}}_{\beta} \left\{a^T\left(\hat{\beta}-\beta\right) + \lambda\left[\left(\hat{\beta}-\beta\right)^T\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) - S\right]\right\},
\end{equation}
$$

where $S = 9.49\hat{\sigma}^2$ and $9.49 = {\chi_4^2}^{\left(1-0.95\right)}$ is the $0.95$ percentile of the chi-squared distribution (see also [Wikipedia: Chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution)).

Differentiating the Lagrangian form in the braces of $\eqref{eq:ex-3.2-beta-maximization}$ with respect to $\lambda$ and $\beta$ and set the derivatives to zero we obtain

$$
\begin{align}
\left(\hat{\beta}-\beta\right)^T\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) - S &= 0 \\
\label{eq:ex-3.2-partial-to-beta}
a + 2\lambda\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) &= 0.
\end{align}
$$

Continue on $\eqref{eq:ex-3.2-partial-to-beta}$,

$$
\begin{align}
\left(\hat{\beta}-\beta\right)^Ta + 2\lambda\left(\hat{\beta}-\beta\right)^T\mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) = 0 \\
2\lambda = -\frac{\left(\hat{\beta}-\beta\right)^Ta}{S} \\
a - \frac{\left(\hat{\beta}-\beta\right)^Ta}{S} \mathbf{X}^T\mathbf{X}\left(\hat{\beta}-\beta\right) = 0 \\
\left(\hat{\beta}-\beta\right)^Ta \left(\hat{\beta}-\beta\right) = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}aS \\
\left[\left(\hat{\beta}-\beta\right)^Ta\right] \left[a^T\left(\hat{\beta}-\beta\right)\right] = a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}aS \\
a^T\left(\hat{\beta}-\beta\right) = \sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}aS},
\end{align}
$$

we reach

$$
\begin{equation}
a^T\left(\hat{\beta}-\beta^r\right) = \sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}aS} = 3.08\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma}.
\end{equation}
$$

The confidence interval of $a^T\beta$ is then given by

$$
\begin{equation}
C_{a^T\beta}^{\beta-\mathrm{set}} = \left(a^T\hat{\beta}-3.08\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma},\; a^T\hat{\beta}+3.08\sqrt{a^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}a}\hat{\sigma}\right).
\end{equation}
$$

We can see that the band of $C_{a^T\beta}^{\beta-\mathrm{set}}$ is wider than that of $C_{a^T\beta}^l$, it is because that some $\beta$ not in $C_{\beta}$ can still make $a^T\beta$ close to $a^T\hat{\beta}$ enough. Therefore if we are more interested in the confidence of the prediction rather than the confidence of the parameter estimation, we should use $C_{a^T\beta}^l$.

**Code**

Here we give our code of confidence interval calculation and plotting.

```{r, fig.width = 12, fig.height = 9}
set.seed(32)

# ------------------------------------------------------------
# training data generation
# ------------------------------------------------------------

training.size <- 100
df <- training.size - 4

CalcFeatures <- function(x1) {
  x2 <- x1 ^ 2
  x3 <- x1 ^ 3
  cbind(x0=rep(1, length(x1)), x1=x1, x2=x2, x3=x3)
}

error <- rnorm(training.size, sd=2)
beta <- c(rnorm(1), rnorm(1, mean=2), rnorm(1, mean=1), rnorm(1, mean=0))

x <- CalcFeatures(rnorm(training.size))
y <- x %*% beta + error
train.data <- data.frame(x)
train.data$y = y

plot(train.data$x1, train.data$y, main='Confidence Intervals of Cubic of X',
     xlab='input', ylab='response', lwd=2, cex=1)

# ------------------------------------------------------------
# model fitting
# ------------------------------------------------------------

lm.fit <- lm(y ~ x1+x2+x3, data=train.data)
beta.hat <- coef(lm.fit)

x.range <- seq(min(train.data$x1), max(train.data$x1), length.out=100)
test.x <- CalcFeatures(x.range)
prediction.y <- predict(lm.fit, as.data.frame(test.x), interval='confidence')
fitted.values <- as.data.frame(prediction.y)

# ------------------------------------------------------------
# confidence bands calculation
# ------------------------------------------------------------

test.data <- data.frame(test.x)

sigma.hat <- sqrt(sum(lm.fit$residuals^2) / df)
sample.cov <- t(x) %*% x
inv.sample.cov <- solve(sample.cov)
t.quantile <- qt(0.975, df)
chi.quantile <- qchisq(0.95, 4)

bands <- apply(test.data, 1, function(x) {
  fitted <- x %*% beta.hat
  factor <- sqrt(t(x) %*% inv.sample.cov %*% x) * sigma.hat
  linear.band <- factor * t.quantile
  beta.set.band <- factor * sqrt(chi.quantile)
  c(fitted=fitted, linear.band=linear.band, beta.set.band=beta.set.band)
})
bands <- as.data.frame(t(bands))

AssertEqual <- function(expected, actual) {
  stopifnot(abs(expected - actual) < 1e-7)
}

AssertEqual(fitted.values$fit, bands$fitted)
AssertEqual(fitted.values$upr - fitted.values$fit, bands$linear.band)

# ------------------------------------------------------------
# boundary beta curves
# ------------------------------------------------------------

svd.decomposed <- svd(sample.cov)
d.sqrt <- matrix(0, ncol=4, nrow=4)
for (i in 1:4) {
  d.sqrt[i, i] <- sqrt(svd.decomposed$d[i])
}
a.matrix <- d.sqrt %*% t(svd.decomposed$v)
inv.a.matrix <- solve(a.matrix)
length.bound <- sigma.hat * sqrt(chi.quantile)

SampleBoundaryBeta <- function() {
  direction <- rnorm(4)
  circle.point <- direction / sqrt(sum(direction^2)) * length.bound
  inv.a.matrix %*% circle.point + beta.hat
}

DrawBoundaryBeta <- function(beta) {
  lines(test.data$x1, test.x %*% beta,
        col='darkgray', lty='dotted', lwd=1)
}

for (i in 1:100) {
  beta.sampled <- SampleBoundaryBeta()
  DrawBoundaryBeta(beta.sampled)
}

# ------------------------------------------------------------
# draw other more important curves
# ------------------------------------------------------------

lines(test.data$x1, bands$fitted - bands$beta.set.band,
      col='blue', lty='longdash', lwd=2)
lines(test.data$x1, bands$fitted + bands$beta.set.band,
      col='blue', lty='longdash', lwd=2)
lines(test.data$x1, fitted.values$lwr,
      col='purple', lty='longdash', lwd=2)
lines(test.data$x1, fitted.values$upr,
      col='purple', lty='longdash', lwd=2)
lines(test.data$x1, fitted.values$fit,
      col='green', lty='solid', lwd=2)
lines(test.data$x1, test.x %*% beta,
      col='red', lty='solid', lwd=2)

```

In the figure above, the plots are the training data points, the red solid line the true model curve (excluding noise), the green solid line the linear regression fit.

The purple dashed lines denote the confidence band which is got from the ```lm``` fit. That confidence band is also calculated in our code and verified against what we get from the ```lm``` fit. Note that we use the percentile of Student's $t$-distribution instead of the percentile of the standard normal distribution in order to accurately match the ```lm``` fit result.

The blue dashed lines denote the confidence band based on the confidence set of $\beta$. It is wider than the normal confidence interval. The dark gray dotted lines are got by using the $\beta$ at the boundary of the confidence set of $\beta$. It can been see that they are well bounded by the blue confidence band but are not bounded by the purple one.

The sampling of the boundary $\beta$ is a little bit tricky. We use the SVD of $\mathbf{X}^T\mathbf{X}$ to achieve that.

---

## Ex. 3.4 least-squares-by-qr

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/1

**Ex. 3.4** Show how the vector of least squares coefficients can be obtained from a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Represent your solution in terms of the QR decomposition of $\mathbf{X}$.

**Solution**

In the beginning, we initalize $\mathbf{z}_0 = \mathbf{x}_0 = \mathbf{1}$. Regressing $\mathbf{x}_1$ on $\mathbf{z}_0$ we get the coefficient $\hat{\gamma}_{01} = \left<\mathbf{z}_0, \mathbf{x}_j\right>\left/\right.\left<\mathbf{z}_0, \mathbf{z}_0\right>$ and the residual $\mathbf{z}_1 = \mathbf{x}_1 - \hat{\gamma}_{01}\mathbf{z}_0$ and hence $\mathbf{x}_1 = \hat{\gamma}_{01}\mathbf{z}_0 + \mathbf{z}_1$. Continue on this procedure, we can get $\mathbf{x}_2 = \hat{\gamma}_{02}\mathbf{z}_0 + \hat{\gamma}_{12}\mathbf{z}_1 + \mathbf{z}_2$, $\mathbf{x}_3 = \hat{\gamma}_{03}\mathbf{z}_0 + \hat{\gamma}_{13}\mathbf{z}_1 + \hat{\gamma}_{23}\mathbf{z}_2 + \mathbf{z}_3$, ..., and $\mathbf{x}_p = \hat{\gamma}_{0p}\mathbf{z}_0 + \hat{\gamma}_{1p}\mathbf{z}_1 + \dots + \hat{\gamma}_{\left(p-1\right)p}\mathbf{z}_{p-1} + \mathbf{z}_p$.

Rewriting equations above with matrix operations we get

$$
\begin{equation}
\mathbf{X} = \mathbf{Z}\mathbf{\Gamma},
\end{equation}
$$

where $\mathbf{z}_j$ are orthogonal of each other and $\mathbf{\Gamma}$ is an upper triangular matrix with all diagonal entries be 1. It is convenient to make length of the orthogonal vectors be 1, so we do it as

$$
\begin{align}
\mathbf{X}
&= \mathbf{Z}\mathbf{\Gamma} \\
&= \mathbf{Z}\mathbf{D}^{-1}\mathbf{D}\mathbf{\Gamma} \\
&= \mathbf{Q}\mathbf{R},
\end{align}
$$

where $\mathbf{D}$ is a diagonal matrix with $j$th diagonal entry $D_{jj} = \left\Vert \mathbf{z}_j \right\Vert$. This way, we make $\mathbf{Q}$ an orthogonal matrix. $\mathbf{R}$ is still an upper triangular matrix but its diagonal entry are now $r_{jj} = \left\Vert \mathbf{z}_j \right\Vert$.

The vector of least square coefficients can be represented by $\mathbf{Q}$ and $\mathbf{R}$:

$$
\begin{align}
\hat{\beta}
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y} \\
&= \left(\mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{R}\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y} \\
&= \left(\mathbf{R}^T\mathbf{R}\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y} \\
&= \mathbf{R}^{-1}\left(\mathbf{R}^T\right)^{-1}\mathbf{R}^T\mathbf{Q}^T\mathbf{y} \\
&= \mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}.
\end{align}
$$

$\mathbf{R}^{-1}$ is also an upper triangular matrix and its entries can be sovled by

$$
\begin{align}
r_{jj}^{-} &= \frac{1}{r_{jj}} \\
r_{kj}^{-} &= -\frac{\sum_{l=k+1}^j r_{kl}r_{lj}^{-}}{r_{kk}},\;k=j-1,\dots,0;
\end{align}
$$

starting from column $0$ to column $p$ and for column $j$ starting from $r_{jj}^{-}$ to $r_{0j}^{-}$.

---

## Ex. 3.5 ridge-inputs-centered

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/2

**Ex. 3.5** Consider the ridge regression problem (3.41). Show that this problem is equivalent to the problem

$$
\begin{equation}
\hat{\beta}^c = \mathop{\mathrm{argmin}}_{\beta^c}\left\{\sum_{i=1}^N\left[y_i-\beta_0^c-\sum_{j=1}^p\left(x_{ij}-\bar{x}_j\right)\beta_j^c\right]^2 + \lambda\sum_{j=1}^p{\beta_j^c}^2\right\}.
\end{equation}
$$

Give the correspondence between $\beta^c$ and the original $\beta$ in (3.41). Characterize the solution to this modified criterion. Show that a similar result holds for the lasso.

**Solution**

Rewriting the optimizating criterion

$$
\begin{align}
RSS^c
&= \sum_{i=1}^N\left[y_i-\beta_0^c-\sum_{j=1}^p\left(x_{ij}-\bar{x}_j\right)\beta_j^c\right]^2 + \lambda\sum_{j=1}^p{\beta_j^c}^2 \\
&= \sum_{i=1}^N\left[y_i-\left(\beta_0^c-\sum_{j=1}^p\bar{x}_j\beta_j^c\right)-\sum_{j=1}^px_{ij}\beta_j^c\right]^2 + \lambda\sum_{j=1}^p{\beta_j^c}^2,
\end{align}
$$

we can see that the correspondence between $\beta^c$ and the original $\beta$ in (3.41) is

$$
\begin{equation}
\left\{
\begin{aligned}
\beta_j &= \beta_j^c, & 1 \le j \le p \\
\beta_0 &= \beta_0^c-\sum_{j=1}^p\bar{x}_j\beta_j^c,
\end{aligned}
\right.
\end{equation}
$$

or equivalently

$$
\begin{equation}
\left\{
\begin{aligned}
\beta_j^c &= \beta_j, & 1 \le j \le p \\
\beta_0^c &= \beta_0+\sum_{j=1}^p\bar{x}_j\beta_j.
\end{aligned}
\right.
\end{equation}
$$

We now proceed to show that $\beta_0^c=\bar{y}=\frac{1}{N}\sum_1^Ny_i$ as stated on page 64. Differentiating $RSS^c$ with respect to $\beta_0^c$ and setting the derivative to zero,

$$
\begin{align}
0
&= \frac{\partial{RSS^c}}{\partial{\beta_0^c}} \\
&= 2\sum_{i=1}^N\left[\beta_0^c-y_i+\sum_{j=1}^p\left(x_{ij}-\bar{x}_j\right)\beta_j^c\right] \\
&= 2N\beta_0^c - 2\sum_{i=1}^Ny_i + 2\sum_{i=1}^N\sum_{j=1}^p\left(x_{ij}-\bar{x}_j\right)\beta_j^c \\
&= 2N\beta_0^c - 2\sum_{i=1}^Ny_i + 2\sum_{j=1}^p\beta_j^c\sum_{i=1}^N\left(x_{ij}-\bar{x}_j\right) \\
&= 2N\beta_0^c - 2\sum_{i=1}^Ny_i + 2\sum_{j=1}^p\beta_j^c\times0 \\
&= 2N\beta_0^c - 2\sum_{i=1}^Ny_i,
\end{align}
$$

we then reach the conclusion we want to draw: $\beta_0^c=\bar{y}=\frac{1}{N}\sum_1^Ny_i$.

Apparently, a similar result holds for the lasso.

---

## Ex. 3.6 ridge-as-posterior-mean

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/2
Reference | [Wikipedia: Multivariate normal distribution][wikipedia-multivariate-normal-distribution]

[wikipedia-multivariate-normal-distribution]: https://en.wikipedia.org/wiki/Multivariate_normal_distribution

**Ex. 3.6** Show that the ridge regression estimate is the mean (and mode) of the posterior distribution, under a Gaussian prior $\beta \sim N\left(0, \tau^2I\right)$, and Gaussian sampling model $\mathbf{y} \sim N\left(\mathbf{X}β, \sigma^2I\right)$. Find the relationship between the regularization parameter $\lambda$ in the ridge formula, and the variances $\tau^2$ and $\sigma^2$.

**Proof**

First we calculate the log likelihood of the training data:

$$
\begin{align}
\mathcal{L}\left(\beta\right)
&= \left(2\pi\right)^{-\frac{N}{2}}\left|\sigma^2I\right|^{-\frac{1}{2}} \mathrm{exp}\left(-\frac{1}{2}\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\sigma^2I\right)^{-1}\left(\mathbf{y}-\mathbf{X}\beta\right)\right) \\
&= \left(2\pi\right)^{-\frac{N}{2}}\sigma^{-N} \mathrm{exp}\left(-\frac{1}{2\sigma^2}\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right)\right), \\
-\log\mathcal{L}\left(\beta\right)
&= \frac{1}{2\sigma^2}\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + C_1.
\end{align}
$$

The minus log prior of $\beta$ can also be calculated as:

$$
\begin{align}
p\left(\beta\right)
&= \left(2\pi\right)^{-\frac{p}{2}}\left|\tau^2I\right|^{-\frac{1}{2}} \mathrm{exp}\left(-\frac{1}{2}\beta^T\left(\tau^2I\right)^{-1}\beta\right) \\
&= \left(2\pi\right)^{-\frac{p}{2}}\tau^{-p} \mathrm{exp}\left(-\frac{1}{2\tau^2}\beta^T\beta\right), \\
-\log p\left(\beta\right)
&= \frac{1}{2\tau^2}\beta^T\beta + C_2.
\end{align}
$$

The posterior density function is proportional to the likelihood of the training data multiplying the prior density function,

$$
\begin{equation}
p\left(\beta|\mathbf{y}\right) \propto \mathcal{L}\left(\beta\right) \times p\left(\beta\right),
\end{equation}
$$

hence the minus log posterior density function will be

$$
\begin{align}
-\log p\left(\beta|\mathbf{y}\right)
&= -\log\mathcal{L}\left(\beta\right) -\log p\left(\beta\right) + C_3 \\
&= \frac{1}{2\sigma^2}\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \frac{1}{2\tau^2}\beta^T\beta + C_1 + C_2 + C_3 \\
&= \frac{1}{2\sigma^2}\left[\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \frac{\sigma^2}{\tau^2}\beta^T\beta\right] + C_4 \\
&= \frac{1}{2\sigma^2}\left[\left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \lambda\beta^T\beta\right] + C_4.
\end{align}
$$

Therefore, minimizing the ridge loss function in (3.43) is equivalent to maximizing the log posterior and the relationship between $\lambda$, $\sigma^2$ and $\tau^2$ is $\lambda=\sigma^2\left/\right.\tau^2$.

The mode (maximun) of the posterior distribution is also the mean, because the log likelihood is a quadratic function of $\beta$.

---

## Ex. 3.7 suspicious-beta-0-penalty

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/5

**Ex. 3.7** Assume $y_i \sim N\left(\beta_0 + x_i^T\beta, \sigma^2\right)$, $i=1,2,\dots,N$, and the parameters $\beta_j$ are each distributed as $N\left(0, \tau^2\right)$, independently of one another. Assuming $\sigma^2$ and $\tau^2$ are known, show that the (minus) log-posterior density of $\beta$ is proportional to $\sum_{i=1}^N\left(y_i − \beta_0 − \sum_jx_{ij}\beta_j\right)^2 + \lambda\sum_{j=1}\beta_j^2$ where $\lambda = \sigma^2/\tau^2$.

**Solution**

We suspect that this exercise has something wrong.

The difference between **Ex. 3.6** and **Ex. 3.7**:

1. **Ex. 3.6**: As shown in **Ex. 3.5**, when we center the inputs the intercept fit is just the average of the response $\beta_0^c=\bar{y}=\frac{1}{N}\sum_1^Ny_i$, and we can proceed to fit the remaining coefficients all with shrinkage. We think that is what **Ex. 3.6** is based on.

1. **Ex. 3.7**: The original inputs are not centered and the penalty is not applied to $\beta_0$. However it seems that $\beta_0$ also follows a prior normal distribution with variance $\tau^2$. That is contradictory.

In order to make the exercise correct, we need to change the exercise by removing the prior distribution of $\beta_0$ like this:

> ... the parameters $\beta_j$ are each distributed as $N\left(0, \tau^2\right)$, $1 \le j \le p$, independently of one another. ...

Or, equivalently, we keep the assumption that $\beta_0 \sim N\left(0, \tau_0^2\right)$ but with $\tau_0^2$ sufficiently large so that $\sigma^2/\tau_0^2 \rightarrow 0$. Once we make this change, then the solution derivation of **Ex. 3.6** will apply to this exercise exatctly.

---

## Ex. 3.8 qr-svd-relationship

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/2
Reference | [Weatherwax and Epstein's solution][weatherwax-epstein-solution]

[weatherwax-epstein-solution]: http://waxworksmath.com/Authors/G_M/Hastie/WriteUp/weatherwax_epstein_hastie_solutions_manual.pdf

**Ex. 3.8** Consider the QR decomposition of the uncentered $N \times (p+1)$ matrix $\mathbf{X}$ (whose first column is all ones), and the SVD of the $N \times p$ centered matrix $\tilde{\mathbf{X}}$. Show that $\mathbf{Q}_2$ and $\mathbf{U}$ span the same subspace, where $\mathbf{Q}_2$ is the sub-matrix of $\mathbf{Q}$ with the first column removed. Under what circumstances will they be the same, up to sign flips?

**Proof**

By the definition of QR decomposition, columns of $\mathbf{Q}$ are orthogonal to each other, then $\mathbf{1}$, proportional to the first column $\mathbf{q}_0$ of $\mathbf{Q}$, is orthogonal to the space spaned by $\mathbf{Q}_2$. By the definition of $\tilde{\mathbf{X}}$, the sum of rows of it produces a $p$ row vector of zeros, in other words, $\mathbf{1}$ is orthogonal to the space spaned by $\tilde{\mathbf{X}}$ and hence $\mathbf{U}$. The two are $p$ dimensional subspaces of the column space of $\mathbf{X}$ and they are both orthogonal to the same vector $\mathbf{1}$, then they are the same subspace.

The following is just a rephrasing of [Weatherwax and Epstein's solution][weatherwax-epstein-solution].

Before anwsering the last question, we need to first show that

$$
\begin{equation}
\mathbf{Q}_2\mathbf{R}_2 = \mathbf{U}\mathbf{D}\mathbf{V}^T,
\end{equation}
$$

where

$$
\begin{align}
\mathbf{X} &= \mathbf{Q}\mathbf{R} = \left[\begin{array}{cc} \mathbf{q}_0 & \mathbf{Q}_2 \end{array}\right] \left[\begin{array}{cc} r_{00} & \mathbf{r}_{0-}^T \\ \mathbf{0} & \mathbf{R}_2 \end{array}\right], \\
\tilde{\mathbf{X}} &= \mathbf{U}\mathbf{D}\mathbf{V}^T.
\end{align}
$$

Looking the $j$th vector of $\mathbf{X}$ we can get the values of the first row of $\mathbf{R}$ as below

$$
\begin{align}
\mathbf{x}_j
&= r_{0j}\mathbf{q}_0 + \sum_{k=1}^jr_{kj}\mathbf{q}_k, \\
\mathbf{1}^T\mathbf{x}_j
&= r_{0j}\mathbf{1}^T\mathbf{q}_0 + \sum_{k=1}^jr_{kj}\mathbf{1}^T\mathbf{q}_k, \\
&= \sqrt{N}r_{0j}, \\
r_{0j} &= \sqrt{N}\bar{\mathbf{x}}_j,
\end{align}
$$

where $\mathbf{q}_0 = \mathbf{1}\left/\right.\sqrt{N}$. Then,

$$
\begin{align}
\left[\begin{array}{cccc} \mathbf{x}_1 & \mathbf{x}_2 & \cdots & \mathbf{x}_p \end{array}\right]
&= \mathbf{q}_0\mathbf{r}_{0-}^T + \mathbf{Q}_2\mathbf{R}_2 \\
&= \left[\begin{array}{cccc} \bar{\mathbf{x}}_1 & \bar{\mathbf{x}}_2 & \cdots & \bar{\mathbf{x}}_p \\ \bar{\mathbf{x}}_1 & \bar{\mathbf{x}}_2 & \cdots & \bar{\mathbf{x}}_p \\ \cdots & \cdots & \cdots & \cdots \\ \bar{\mathbf{x}}_1 & \bar{\mathbf{x}}_2 & \cdots & \bar{\mathbf{x}}_p \end{array}\right] + \mathbf{Q}_2\mathbf{R}_2, \\
\mathbf{Q}_2\mathbf{R}_2
&= \tilde{\mathbf{X}} = \mathbf{U}\mathbf{D}\mathbf{V}^T.
\end{align}
$$

When $\mathbf{Q}_2$ and $\mathbf{U}$ are the same, up to sign flips and even column permutations, we have $\mathbf{R}_2 = \mathbf{Q}_2^T\mathbf{U}\mathbf{D}\mathbf{V}^T$, where $\mathbf{Q}_2^T\mathbf{U}$ is a row permutation of $\mathrm{diag}\left(e_1, e_2, \cdots, e_p\right)$ where $e_j = 1$ or $e_j = -1$ for $1 \le j \le p$. Because $\mathbf{V}^T$ is row orthogonal, the upper triangular matrix $\mathbf{R}_2$ is also row orthogonal.

If an upper triangular matrix is row orthogonal, then it is diagonal. Let us look at the inner product between the last row vector and any of the rest row vectors, we can find the non-zero $r_{pp}$ will clear all last member of other row vectors.

$$
\begin{align}
0 &= \left<\mathbf{r}_p^T,\mathbf{r}_j^T\right> = r_{pp}r_{jp}, \\
r_{jp} &= 0.
\end{align}
$$

Continue this procedure, we can find that all upper triangular entries except the diagonal ones are cleared to zeros.

Finally, we reach the conclusion that $\mathbf{R}_2$ is diagonal if $\mathbf{Q}_2$ and $\mathbf{U}$ are the same, up to sign flips and even column permutations.

---

## Ex. 3.9 forward-stepwise-regression

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/2

**Ex. 3.9** _Forward stepwise regression._ Suppose we have the QR decomposition for the $N \times q$ matrix $\mathbf{X}_1$ in a multiple regression problem with response $\mathbf{y}$, and we have an additional $p−q$ predictors in the matrix $\mathbf{X}_2$. Denote the current residual by $\mathbf{r}$. We wish to establish which one of these additional variables will reduce the residual-sum-of squares the most when included with those in $\mathbf{X}_1$. Describe an efficient procedure for doing this.

**Solution**

Under current QR decomposition

$$
\begin{equation}
\mathbf{X}_1 = \mathbf{Q}_1\mathbf{R}_1,
\end{equation}
$$

the current residual is

$$
\begin{equation}
\label{eq:ex-3.9-residual}
\mathbf{r} = \mathbf{y} - \mathbf{Q}_1\mathbf{Q}_1^T\mathbf{y}.
\end{equation}
$$

The additional variable in $\mathbf{X}_2$ which will reduce the RSS the most is the one whose correlation with $\mathbf{r}$ is the biggest:

$$
\begin{equation}
\label{eq:ex-3.9-correlation}
\rho_{\mathbf{r},\mathbf{x}_j} = \frac{\left<\mathbf{r}-\bar{\mathbf{r}}, \mathbf{x}_j-\bar{\mathbf{x}}_j\right>}{\left.\middle\Vert\mathbf{r}\middle\Vert \cdot \middle\Vert\mathbf{x}_j\middle\Vert\right.}.
\end{equation}
$$

In addition to QR decomputation, the main computation costs which the step $k$ of this procedure involves are:

1. Correlations between $p-k+1$ candidate variables and the current residual $\mathbf{r}$ as in $\eqref{eq:ex-3.9-correlation}$. In fact we do not need to divide the covariance by $\left\Vert\mathbf{r}\right\Vert$ because we only need to know the order. Also, standardizing the inputs beforehand can also save many redundant computations.
1. Inner products between $\mathbf{y}$ and the newly introduced $\mathbf{q}_j$, scala-vector multiplications and vector summations as in $\eqref{eq:ex-3.9-residual}$.

---

## Ex. 3.10 backward-stepwise-regression

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/4

**Ex. 3.10** Backward stepwise regression. Suppose we have the multiple regression fit of $\mathbf{y}$ on $\mathbf{X}$, along with the standard errors and Z-scores as in Table 3.2. We wish to establish which variable, when dropped, will increase the residual sum-of-squares the least. How would you do this?

**Solution**

For conviniences we paste Table 3.2 as below.

Term | Coefficient | Std. Error | Z Score
--- | --- | --- | ---
Intercept | 2.46 | 0.09 | 27.60
lcavol | 0.68 | 0.13 | 5.37
lweight | 0.26 | 0.10 | 2.75
age | −0.14 | 0.10 | −1.40
lbph | 0.21 | 0.10 | 2.06
svi | 0.31 | 0.12 | 2.47
lcp | −0.29 | 0.15 | −1.87
gleason | −0.02 | 0.15 | −0.15
pgg45 | 0.27 | 0.15 | 1.74

On page (49) of the book and also in solution to Ex. 3.1, we have shown that the $F$ statistic (3.13) for dropping a single coefficient from a model is equal to the square of the corresponding $z$-score (3.12), where the $F$ statistic for dropping a single coefficient is the RSS increament divided by $\hat{\sigma}^2$. Therefore, for backward stepwise regression we should drop the variable with the smallest absolute $z$-score. In our case in Table 3.2, it is the variable **gleason**.

---

## Ex. 3.11 multivariate-linear-regression

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/4

**Ex. 3.11** Show that the solution to the multivariate linear regression problem (3.40) is given by (3.39). What happens if the covariance matrices $\mathbf{\Sigma}_i$ are different for each observation?

**Proof**

If the errors $\epsilon = \left(\epsilon_1, \dots, \epsilon_K\right)$ are correlated with $\mathrm{Cov}\left(\epsilon\right) = \mathbf{\Sigma}$, then the multivariate weighted criterion

$$
\begin{equation}
RSS\left(\mathbf{B},\mathbf{\Sigma}\right) = \sum_{i=1}^N\left(y_i-f\left(x_i\right)\right)^T\mathbf{\Sigma}^{-1}\left(y_i-f\left(x_i\right)\right)
\end{equation}
$$

is used in finding the fit, where $f\left(x\right)$ is the vector function $\left(f_1\left(x\right), \dots, f_K\left(x\right)\right)^T$.

In what follows, we will follow [IFEM.AppF.pdf: Matrix Calculus](http://www.colorado.edu/engineering/cas/courses.d/IFEM.d/IFEM.AppF.d/IFEM.AppF.pdf) for the definition of derivatives of vectors and matrices.

Starting from the derative of a column of coefficient $\mathbf{\beta}_k$,

$$
\begin{align}
\frac{\partial RSS\left(\mathbf{B},\mathbf{\Sigma}\right)}{\partial \mathbf{\beta}_k}
&= \sum_{i=1}^N\frac{\partial}{\partial \mathbf{\beta}_k}\left(y_i-f\left(x_i\right)\right)^T\mathbf{\Sigma}^{-1}\left(y_i-f\left(x_i\right)\right) \\
&= \sum_{i=1}^N\left[\frac{\partial}{\partial f_k\left(x_i\right)}\left(y_i-f\left(x_i\right)\right)^T\mathbf{\Sigma}^{-1}\left(y_i-f\left(x_i\right)\right)\right] \frac{\partial f_k\left(x_i\right)}{\partial \mathbf{\beta}_k} \\
&= -2\sum_{i=1}^N\left[\left(y_i-f\left(x_i\right)\right)^T{\mathbf{\Sigma}^{-1}}_k\right] x_i \\
&= -2\sum_{i=1}^Nx_i \left[\left(y_i-f\left(x_i\right)\right)^T{\mathbf{\Sigma}^{-1}}_k\right],
\end{align}
$$

where ${\mathbf{\Sigma}^{-1}}_k$ denotes the $k$th column of $\mathbf{\Sigma}^{-1}$, we get the full derivative of $\mathbf{B}$:

$$
\begin{align}
\frac{\partial RSS\left(\mathbf{B},\mathbf{\Sigma}\right)}{\partial \mathbf{B}}
&= \left[\begin{array}{ccc} \frac{\partial}{\partial \mathbf{\beta}_1} & \dots & \frac{\partial}{\partial \mathbf{\beta}_K} \end{array}\right] RSS\left(\mathbf{B},\mathbf{\Sigma}\right) \\
&= -2\sum_{i=1}^Nx_i \left[\begin{array}{ccc} \left(y_i-f\left(x_i\right)\right)^T{\mathbf{\Sigma}^{-1}}_1 & \dots & \left(y_i-f\left(x_i\right)\right)^T{\mathbf{\Sigma}^{-1}}_K \end{array}\right] \\
\label{eq:ex-3.11-separate-derivative}
&= -2\sum_{i=1}^Nx_i \left(y_i-f\left(x_i\right)\right)^T\mathbf{\Sigma}^{-1}.
\end{align}
$$

Because $\mathbf{\Sigma}$ is common to all observations, then $\eqref{eq:ex-3.11-separate-derivative}$ can be _piled_ as matrix multiplications

$$
\begin{align}
\frac{\partial RSS\left(\mathbf{B},\mathbf{\Sigma}\right)}{\partial \mathbf{B}}
&= -2\sum_{i=1}^Nx_i \left(y_i-f\left(x_i\right)\right)^T\mathbf{\Sigma}^{-1} \\
\label{eq:ex-3.11-same-sigma}
&= -2\sum_{i=1}^Nx_i \left(y_i-\mathbf{B}^Tx_i\right)^T\mathbf{\Sigma}^{-1} \\
\label{eq:ex-3.11-piling}
&= -2\mathbf{X}^T\left(\mathbf{Y}-\mathbf{X}\mathbf{B}\right)\mathbf{\Sigma}^{-1}.
\end{align}
$$

Setting the derivative to zero, we finally get

$$
\begin{align}
-2\mathbf{X}^T\left(\mathbf{Y}-\mathbf{X}\hat{\mathbf{B}}\right)\mathbf{\Sigma}^{-1} & = \mathbf{0}, \\
\label{eq:ex-3.11-sigma-cancelled}
\mathbf{X}^T\left(\mathbf{Y}-\mathbf{X}\hat{\mathbf{B}}\right) & = \mathbf{0}, \\
\mathbf{X}^T\mathbf{Y} & = \mathbf{X}^T\mathbf{X}\hat{\mathbf{B}}, \\
\hat{\mathbf{B}} &= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{Y}.
\end{align}
$$

Equation $\eqref{eq:ex-3.11-sigma-cancelled}$ holds because $\mathbf{\Sigma}^{-1}$ is full rank.

This result shows that if the errors are _statically_ correlated (the correlation does not vary with observations), the solution of the coefficients are the same as in the uncorrelated scenario, as if the correlations are ignored.

If the covariance matrices $\mathbf{\Sigma}_i$ are different for each observation, the common $\mathbf{\Sigma}^{-1}$ will be replaced with $\mathbf{\Sigma}_i^{-1}$ in $\eqref{eq:ex-3.11-same-sigma}$, hence we cannot get the simplified _piled_ matrix operation as in $\eqref{eq:ex-3.11-piling}$.

---

## Ex. 3.12 ridge-by-artificial-data

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/5

**Ex. 3.12** Show that the ridge regression estimates can be obtained by ordinary least squares regression on an augmented data set. We augment the centered matrix $\mathbf{X}$ with $p$ additional rows $\sqrt{\lambda}\mathbf{I}$, and augment $\mathbf{y}$ with $p$ zeros. By introducing artificial data having response value zero, the fitting procedure is forced to shrink the coefficients toward zero. This is related to the idea of _hints_ due to Abu-Mostafa (1995), where model constraints are implemented by adding artificial data examples that satisfy them.

**Proof**

The proof is quite straightforward by rewriting (3.43) on page 64:

$$
\begin{align}
RSS\left(\lambda\right)
&= \left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \lambda\beta^T\beta \\
&= \left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \left(\sqrt{\lambda}\beta\right)^T\left(\sqrt{\lambda}\beta\right) \\
&= \left(\mathbf{y}-\mathbf{X}\beta\right)^T\left(\mathbf{y}-\mathbf{X}\beta\right) + \left(\mathbf{0} - \sqrt{\lambda}\mathbf{I}\beta\right)^T\left(\mathbf{0} - \sqrt{\lambda}\mathbf{I}\beta\right) \\
&= \left(\mathbf{y}_+-\mathbf{X}_+\beta\right)^T\left(\mathbf{y}_+-\mathbf{X}_+\beta\right),
\end{align}
$$

where

$$
\begin{align}
\mathbf{y}_+ &= \left[\begin{array}{c} \mathbf{y} \\ \mathbf{0} \end{array}\right] \\
\mathbf{X}_+ &= \left[\begin{array}{c} \mathbf{X} \\ \sqrt{\lambda}\mathbf{I} \end{array}\right].
\end{align}
$$

---

## Ex. 3.13 pcr-as-x-coefficients

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/5

**Ex. 3.13** Derive the expression (3.62), and show that $\hat{\beta}^{\mathrm{pcr}}\left(p\right) = \hat{\beta}^{\mathrm{ls}}$.

**Proof**

Similar to what we have shown in **Ex. 3.5**, when the input variables are centered the intercept fit is the mean of the response variable. We just need to fit the remaining coefficients. The SVD of the $N \times p$ matrix $\mathbf{X}$ is $\mathbf{X} = \mathbf{U}\mathbf{D}\mathbf{V}^T$, the principal component regression fits the model by considering only the first $M$ derived input columns $\mathbf{z}_m = \mathbf{X}v_m$, $M \le p$, $1 \le m \le M$. The coefficients of $\mathbf{z}_m = \mathbf{X}v_m$ are given by $\hat{\theta}_m = \left<\mathbf{z}_m, \mathbf{y}\right> \left/\right. \left<\mathbf{z}_m, \mathbf{z}_m\right>$.

What we need to prove is that the solution of

$$
\begin{equation}
\hat{\mathbf{y}}_{\left(M\right)}^{\mathrm{pcr}} = \bar{y}\mathbf{1}+\sum_{m=1}^M\hat{\theta}_m\mathbf{z}_m
\end{equation}
$$

can be expressed in terms of coefficients of the $\mathbf{x}_j$:

$$
\begin{equation}
\label{eq:ex-3.12-pcr-coefficients}
\hat{\beta}^{\mathrm{pcr}}\left(M\right) = \sum_{m=1}^M\hat{\theta}_mv_m.
\end{equation}
$$

Let's simplify the terms:

$$
\begin{align}
\mathbf{z}_m &= \mathbf{X}v_m = \mathbf{U}\mathbf{D}\mathbf{V}^Tv_m = d_mu_m, \\
\hat{\theta}_m &= \frac{\left<\mathbf{z}_m, \mathbf{y}\right>}{\left<\mathbf{z}_m, \mathbf{z}_m\right>} = \frac{\mathbf{z}_m^T\mathbf{y}}{\mathbf{z}_m^T\mathbf{z}_m} = \frac{d_mu_m^T\mathbf{y}}{d_m^2u_m^Tu_m} = \frac{1}{d_m}u_m^T\mathbf{y}, \\
\sum_{m=1}^M\hat{\theta}_m\mathbf{z}_m &= \sum_{m=1}^M\left(\frac{1}{d_m}u_m^T\mathbf{y}\right)\left(d_mu_m\right) = \sum_{m=1}^Mu_mu_m^T\mathbf{y} = \mathbf{U}_M\mathbf{U}_M^T\mathbf{y}, \\
\end{align}
$$

where $d_m$ is the $d$th diagonal entry of $\mathbf{D}$, and $\mathbf{U}_M$ is a $N \times M$ matrix containing the first left $M$ columns of $\mathbf{U}$.

Continue on simplifying $\eqref{eq:ex-3.12-pcr-coefficients}$ we get

$$
\begin{equation}
\sum_{m=1}^M\hat{\theta}_mv_m = \sum_{m=1}^M\left(\frac{1}{d_m}u_m^T\mathbf{y}\right)v_m = \sum_{m=1}^M\frac{1}{d_m}v_mu_m^T\mathbf{y} = \mathbf{V}_M\mathbf{D}_M^{-1}\mathbf{U}_M^T\mathbf{y},
\end{equation}
$$

where $\mathbf{V}_M$ denotes a $p \times M$ matrix containing the first left $M$ columns of $\mathbf{V}$, and $\mathbf{D}_M$ denotes a $M \times M$ diagonal matrix containing the first $M$ diagonal entries of $\mathbf{D}$.

Left multiplying the coefficients (yet to be proved) by the inputs matrix,

$$
\begin{align}
\mathbf{X}\sum_{m=1}^M\hat{\theta}_mv_m
&= \mathbf{X}\mathbf{V}_M\mathbf{D}_M^{-1}\mathbf{U}_M^T\mathbf{y} \\
&= \mathbf{U}\mathbf{D}\mathbf{V}^T\mathbf{V}_M\mathbf{D}_M^{-1}\mathbf{U}_M^T\mathbf{y} \\
&= \mathbf{U}\mathbf{D} \left[\begin{array}{c} \mathbf{I} \\ \mathbf{0} \end{array}\right] \mathbf{D}_M^{-1}\mathbf{U}_M^T\mathbf{y} \\
&= \mathbf{U} \left[\begin{array}{c} \mathbf{D}_M \\ \mathbf{0} \end{array}\right] \mathbf{D}_M^{-1}\mathbf{U}_M^T\mathbf{y} \\
&= \mathbf{U} \left[\begin{array}{c} \mathbf{I} \\ \mathbf{0} \end{array}\right] \mathbf{U}_M^T\mathbf{y} \\
&= \mathbf{U}_M\mathbf{U}_M^T\mathbf{y} \\
&= \sum_{m=1}^M\hat{\theta}_m\mathbf{z}_m,
\end{align}
$$

we find that

$$
\begin{align}
\hat{\mathbf{y}}_{\left(M\right)}^{\mathrm{pcr}}
&= \bar{y}\mathbf{1}+\sum_{m=1}^M\hat{\theta}_m\mathbf{z}_m \\
&= \bar{y}\mathbf{1}+\mathbf{X}\sum_{m=1}^M\hat{\theta}_mv_m,
\end{align}
$$

hence the principal component regression coefficients of $\mathbf{z}_m$ can be represented by the coefficients of $\mathbf{x}_j$ as:

$$
\begin{equation}
\label{eq:ex-3.12-coefficients-by-matrix}
\hat{\beta}^{\mathrm{pcr}}\left(M\right) = \sum_{m=1}^M\hat{\theta}_mv_m = \mathbf{V}_M\mathbf{D}_M^{-1}\mathbf{U}_M^T\mathbf{y}.
\end{equation}
$$

Under SVD representation, the least square model fit is

$$
\begin{align}
\hat{\beta}^{\mathrm{ls}}
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y} \\
&= \left(\mathbf{V}\mathbf{D}\mathbf{U}^T\mathbf{U}\mathbf{D}\mathbf{V}^T\right)^{-1}\mathbf{V}\mathbf{D}\mathbf{U}^T\mathbf{y} \\
&= \left(\mathbf{V}\mathbf{D}^2\mathbf{V}^T\right)^{-1}\mathbf{V}\mathbf{D}\mathbf{U}^T\mathbf{y} \\
&= \mathbf{V}\mathbf{D}^{-2}\mathbf{V}^T\mathbf{V}\mathbf{D}\mathbf{U}^T\mathbf{y} \\
&= \mathbf{V}\mathbf{D}^{-2}\mathbf{D}\mathbf{U}^T\mathbf{y} \\
&= \mathbf{V}\mathbf{D}^{-1}\mathbf{U}^T\mathbf{y}.
\end{align}
$$

Compare with $\eqref{eq:ex-3.12-coefficients-by-matrix}$, we get the fact that

$$
\begin{equation}
\hat{\beta}^{\mathrm{pcr}}\left(p\right) = \hat{\beta}^{\mathrm{ls}}.
\end{equation}
$$

Q.E.D.

---

## Ex. 3.14 pls-when-orthogonal

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/11/7

**Ex. 3.14** Show that in the orthogonal case, PLS stops after $m=1$ steps, because subsequent $\hat{\phi}_{mj}$ in step 2 in Algorithm 3.3 are zero.

**Proof**

Rewriting Algorithm 3.3 using vector and matrix operations we get

$$
\begin{align}
\hat{\phi}_{mj} &= {\mathbf{x}_j^{\left(m-1\right)}}^T\mathbf{y}, \\
\hat{\phi}_m &= {\mathbf{X}^{\left(m-1\right)}}^T\mathbf{y}, \\
\mathbf{z}_m &= \sum_{j=1}^p\hat{\phi}_{mj}\mathbf{x}_j^{\left(m-1\right)} = \sum_{j=1}^p\mathbf{x}_j^{\left(m-1\right)}{\mathbf{x}_j^{\left(m-1\right)}}^T\mathbf{y} = \mathbf{X}^{\left(m-1\right)}{\mathbf{X}^{\left(m-1\right)}}^T\mathbf{y}, \\
\hat{\theta}_m &= \frac{\left<\mathbf{z}_m,\mathbf{y}\right>}{\left<\mathbf{z}_m,\mathbf{z}_m\right>} = \frac{\mathbf{y}^T\mathbf{X}^{\left(m-1\right)}{\mathbf{X}^{\left(m-1\right)}}^T\mathbf{y}}{\mathbf{y}^T\mathbf{X}^{\left(m-1\right)}{\mathbf{X}^{\left(m-1\right)}}^T\mathbf{X}^{\left(m-1\right)}{\mathbf{X}^{\left(m-1\right)}}^T\mathbf{y}}, \\
\hat{\mathbf{y}}^{\left(m\right)} &= \hat{\mathbf{y}}^{\left(m-1\right)} + \hat{\theta}_m\mathbf{z}_m, \\
\mathbf{x}_j^{\left(m\right)} &= \mathbf{x}_j^{\left(m-1\right)} - \frac{\left<\mathbf{z}_m,\mathbf{x}_j^{\left(m-1\right)}\right>}{\left<\mathbf{z}_m,\mathbf{z}_m\right>}\mathbf{z}_m = \mathbf{x}_j^{\left(m-1\right)} - \frac{\mathbf{z}_m^T\mathbf{x}_j^{\left(m-1\right)}}{\mathbf{z}_m^T\mathbf{z}_m}\mathbf{z}_m, \\
\mathbf{X}^{\left(m\right)} &= \mathbf{X}^{\left(m-1\right)} - \frac{\mathbf{z}_m\mathbf{z}_m^T\mathbf{X}^{\left(m-1\right)}}{\mathbf{z}_m^T\mathbf{z}_m}.
\end{align}
$$

In $m=1$ steps, when the columns of $\mathbf{X}^{\left(0\right)}$ are orthogonal, i.e., ${\mathbf{X}^{\left(0\right)}}^T\mathbf{X}^{\left(0\right)} = \mathbf{I}$, then

$$
\begin{align}
\hat{\theta}_1
&= \frac{\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}}{\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}} \\
&= \frac{\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}}{\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}} \\
&= 1, \\
\mathbf{X}^{\left(1\right)}
&= \mathbf{X}^{\left(0\right)} - \frac{\mathbf{z}_1\mathbf{z}_1^T\mathbf{X}^{\left(0\right)}}{\mathbf{z}_1^T\mathbf{z}_1} \\
&= \mathbf{X}^{\left(0\right)} - \frac{\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{X}^{\left(0\right)}}{\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}} \\
&= \mathbf{X}^{\left(0\right)} - \frac{\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}\mathbf{y}^T\mathbf{X}^{\left(0\right)}}{\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}}, \\
\hat{\phi}_2
&= {\mathbf{X}^{\left(1\right)}}^T\mathbf{y} \\
&= {\mathbf{X}^{\left(0\right)}}^T\mathbf{y} - \frac{{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}}{\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}} \\
&= {\mathbf{X}^{\left(0\right)}}^T\mathbf{y} - \frac{{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}\left(\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}\right)}{\mathbf{y}^T\mathbf{X}^{\left(0\right)}{\mathbf{X}^{\left(0\right)}}^T\mathbf{y}} \\
&= {\mathbf{X}^{\left(0\right)}}^T\mathbf{y} - {\mathbf{X}^{\left(0\right)}}^T\mathbf{y} \\
&= \mathbf{0}.
\end{align}
$$

PLS will stop after $m=1$ steps because $\hat{\phi}_2=\mathbf{0}$.

Further, because $\hat{\theta}_1=1$, then

$$
\begin{equation}
\hat{\mathbf{y}}=\mathbf{z}_1=\mathbf{X}\mathbf{X}^T\mathbf{y},
\end{equation}
$$

the PLS estimate is $\mathbf{X}^T\mathbf{y}$, which equals to the least squares estimate $\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}=\mathbf{X}^T\mathbf{y}$.
