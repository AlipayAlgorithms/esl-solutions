---
title: "Overview of Supervised Learning"
output:
  html_document:
    toc: yes
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

---

## Note [p:12 para:2 eq:2.5] derivative-scalar-by-vector

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/8/18
Reference | [Wikipedia: Matrix calculus][wikipedia-matrix-calculus]

[wikipedia-matrix-calculus]: https://en.wikipedia.org/wiki/Matrix_calculus

There are 2 _flavors_ of definition of **the derivative of a scalar by a vector** in terms of the form of the resultant vector:

- Row vector: [Wikipedia: Matrix calculus][wikipedia-matrix-calculus], [tirgul3_derivatives.pdf: Derivatives with respect to vectors](http://www.cs.huji.ac.il/~csip/tirgul3_derivatives.pdf)
- Column vector: [IFEM.AppF.pdf: Matrix Calculus](http://www.colorado.edu/engineering/cas/courses.d/IFEM.d/IFEM.AppF.d/IFEM.AppF.pdf), [onlinelibrary: Differentiation with respect to a vector](http://onlinelibrary.wiley.com/doi/10.1002/0471705195.app3/pdf)

This book follows the latter, i.e., the column vector form:

$$
\begin{equation}
\frac{\partial{y}}{\partial{\mathbf{x}}} = \begin{bmatrix} \frac{\partial y}{\partial x_1} \\ \frac{\partial y}{\partial x_2} \\ \vdots \\ \frac{\partial y}{\partial x_n} \end{bmatrix}
\end{equation}
$$

This can also be verified by equation (3.14) on page 45.

---

## Note [p:15 para:-2] knn-degrees-of-freedom

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/8/20
Reference | [Wikipedia: Degrees of freedom][wikipedia-degrees-of-freedom]

[wikipedia-degrees-of-freedom]: https://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29

In the paragraph of interest, it is said that:

> ... we will see that the _effective_ number of parameters of $k$-nearest neighbors is $N/k$ ... To get an idea of why, note that if the neighborhoods were nonoverlapping, there would be $N/k$ neighborhoods and we would fit one parameter (a mean) in each neighborhood.

In order to understand this paragraph quantitatively, we may need to refer to [Wikipedia: Degrees of freedom][wikipedia-degrees-of-freedom] and equation (3.50) on page 68. In many regression tasks, including $k$-nearest-neighbors, the prediction on training data can be given by a linear combination of target values of the training samples:

$$
\begin{equation}
\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}
\end{equation}
$$

A general definition of the **effective degrees of freedom** is given by the trace of the **hat** matrix $\mathbf{H}$:

$$
\begin{equation}
\mathrm{tr}(\mathbf{H}) = \sum_{i} h_{ii}
\end{equation}
$$

In a $k$-nearest-neighbor fit, each row of the hat matrix $\mathbf{H}$ contains exactly $k$ non-zero cells with the value of $1/k$, and the diagonal cell is always $1/k$ because the nearest neighbor of each data sample is of course itself. Therefore, the trace of the hat matrix is naturally $N/k$.

---

## Note [p:16 para:-1] - [p:17 para:1] simulated-gaussian-mixture

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/9/1
Reference | [Wikipedia: Mixture model][wikipedia-mixture-model]

[wikipedia-mixture-model]: https://en.wikipedia.org/wiki/Mixture_model

In section 2.3, the training data were _simulated from a model somewhere between the two, but closer to Scenario 2_.

> **Scenario 1**: The training data in each class were generated from bivariate Gaussian distributions with uncorrelated components and different means.

> **Scenario 2**: The training data in each class came from a mixture of 10 low-variance Gaussian distributions, with individual means themselves distributed as Gaussian.

The sampling procedure described in the paragraph of interest **follows** the definition of Gaussian mixture models of no doubt. The difference between that procedure and Scenario 2 lies in the fact that the two means of the Gaussian distributions from which the individual means of the two classes were drawn **are different**, which are $(1, 0)^T$ and $(0, 1)^T$, respectively. In Scenario 2 however, the individual means follow the same Gaussian distribution. Therefore, the separability of the simulated data lies between Scenario 1 and Scenario 2.

---

## Note [p:23 para:1] high-dimension-extrapolation

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/9/10
Reference | [Wikipedia: Extrapolation][wikipedia-extrapolation]

[wikipedia-extrapolation]: https://en.wikipedia.org/wiki/Extrapolation

In the paragraph of interest, it is said that:

> Hence most data points are closer to the boundary of the sample space than to any other data point. The reason that this presents a problem is that prediction is much more difficult near the edges of the training sample. One must extrapolate from neighboring sample points rather than interpolate between them.

Interpolation and extrapolation correspond to predictions inside and at the boundary of the training region, respectively.

Let's first illustrate the difference in the 1-dimensional space. When we need to predict the $y$ value given a test data point $x$ _inside_ the training region, then we can find approximately $k/2$ training data points at both left and right sides of $x$, then we can predict $y$ by _interpolating_ among the $k$ surrounding training data points. However, when $x$ is at the left _boundary_ of the training region, then we may find that only $k/10$ of the $k$-nearest neighbors are at the left side of $x$, and the rest $9k/10$ are at the right side. Or even more unfortunetaly, all the $k$-nearest neighbors could be at the right side of $x$. Therefore, we are actually _extrapolating_ from the neighbors in order to predict $y$, because $x$ is not well _embraced_ by the $k$-nearest neighbors.

In high dimensional spaces, most points appear at the boundaries. For each $x$ we want to predict, it is extremely highly possible that $x$ lies at the boundary of the training region. Most of the $k$-nearest neighbors of $x$ are just at one side of $x$ hence are not embracing $x$ well. That comes the sentence **"One must extrapolate from neighboring sample points rather than interpolate between them"**.

---

## Ex. 2.1 output-class-index

Item | Description
--- | ---
Authors | [justdark](https://github.com/justdark)
Date | 2015/9/5

**Ex. 2.1** Suppose each of $K$-classes has an associated target $t_{k}$, which is a vector of all zeros, except a one in the $k$th position. Show that classifying to the largest element of $\hat{y}$ amounts to choosing the closest target, $\mathrm{min}_{k}\left|\left|t_{k} - \hat{y}\right|\right|$, if the elements of $\hat{y}$ sum to one.

**Proof**

According to the description, what we need to prove is:
$$
\begin{equation}
\mathop{\mathrm{argmax}}_{k}\left(\hat{y}_k\right)=\mathop{\mathrm{argmin}}_{k}(\left|\left|t_{k}-\hat{y}\right|\right|)
\end{equation}
$$

So we have:
$$
\begin{align}
\mathop{\mathrm{argmin}}_{k}\left(\left|\left|t_{k}-\hat{y}\right|\right|\right) &=  \mathop{\mathrm{argmin}}_{k}\left\|t_{k}-\hat{y}\right\|^2 \\
&= \mathop{\mathrm{argmin}}_{k} \sum_{i=1}^K\left[\left(t_{k}\right)_{i}-\hat{y}_i\right]^2 \\
&= \mathop{\mathrm{argmin}}_{k} \sum_{i=1}^K\left[\left(t_{k}\right)_{i}^2+\hat{y}_i^2-2\left(t_{k}\right)_{i}\hat{y}_i\right]\\
&= \mathop{\mathrm{argmin}}_{k} \sum_{i=1}^K\left[\left(t_{k}\right)_{i}^2-2\left(t_{k}\right)_{i}\hat{y}_i\right] \\
&= \mathop{\mathrm{argmin}}_{k} \left[\sum_{i=1}^K\left(t_{k}\right)_{i}^2-\sum_{i=1}^K2\left(t_{k}\right)_{i}\hat{y}_i\right] \\
&= \mathop{\mathrm{argmin}}_{k} \left[1-\sum_{i=1}^K2\left(t_{k}\right)_{i}\hat{y}_i\right]\\
&= \mathop{\mathrm{argmax}}_{k} \sum_{i=1}^K\left[\left(t_{k}\right)_{i}\hat{y}_i\right]\\
&= \mathop{\mathrm{argmax}}_{k} \left(\hat{y}_k\right)
\end{align}
$$

As you can see, the last condition that the elements of $\hat{y}$ sum to one doesn't appear in our proof, because it's unnecessary for the conclusion.

---

## Ex. 2.2 bayes-decision-boundary

Item | Description
--- | ---
Authors | [justdark](https://github.com/justdark)
Date | 2015/9/8

**Ex. 2.2** Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5.

**Proof**

From the description we can conclude that the boundary's property:
$$
\begin{equation}
Pr\left(\mathcal{G}_{orange}\left|\right.X=x\right)=Pr\left(\mathcal{G}_{blue}\left|\right.X=x\right)
\end{equation}
$$
where x is the point on the boundary. By the Bayes equation:
$$
\begin{equation}
Pr\left(\mathcal{G}_{k}\left|\right.X = x\right) = \frac{Pr\left(X = x\left|\right.\mathcal{G}_{k}\right)Pr\left(\mathcal{G}_{k}\right)}{Pr\left(X=x\right)}
\end{equation}
$$

Because the $Count\left(orange\right)=Count\left(blue\right)$, so
$$
\begin{equation}
Pr\left(\mathcal{G}_{orange}\right)=Pr\left(\mathcal{G}_{blue}\right)
\end{equation}
$$

We can simplify the boundary's property, it's also **the solution of this exercise**:
$$
\begin{align}
Pr\left(X = x\left|\right.\mathcal{G}_{orange}\right)&=Pr\left(X = x\left|\right.\mathcal{G}_{blue}\right)
\end{align}
$$

How to calculate the $Pr\left(X = x\left|\right.\mathcal{G}_{k}\right)$ ?

For the **Scenario 1**, if the $\mathcal{G}$ is simulated from two bivariate Guassian distribution, we can use the **probability density function (pdf)** of Guassian distribution:
$$
\begin{align}
f\left(x\right)=\frac{1}{\sqrt{\left(2\pi\right)^{2}\left|\Sigma\right|}}\exp\left(-\frac{1}{2}({x}-{\mu})^\mathrm{T}{\Sigma}^{-1}({x}-{\mu})\right)
\end{align}
$$
where $\left|\Sigma\right|$ is the determinant of $\Sigma$
Because the $\sigma$ is same for all class, so the boundary is actually **midperpendicular** of line segment from $\mu_{orange}$ to $\mu_{blue}$.

For a more complicated scenario which generate the data in Figure 2.5, each class, we first generate 5 points as the 5 new Guassian distributions' mean, then each distribution generates 20 points as the results, finally we got 100 points for the class.

Use $\left(m_k\right)_t,t\le5$ as the results of the first step for class $k$.
The probability that a point is generate from a given mean $\left(m_k\right)_t$ is:
$$
\begin{align}
f\left(x,\left(m_k\right)_t\right)=\frac{1}{\sqrt{\left(2\pi\right)^{2}\left|\Sigma^{'}\right|}}\exp\left(-\frac{1}{2}\left[{x}-{\left(m_k\right)_t}\right]^\mathrm{T}{\Sigma^{'}}^{-1}\left[{x}-{\left(m_k\right)_t}\right]\right)
\end{align}
$$
(add apostrophe on the $\Sigma$ to distinguish it from basic one)

So we can define the probability that the point is generate from the $k$th class as:
$$
\begin{align}
Pr\left(X = x\left|\right.\mathcal{G}_{k}\right) = \frac{1}{5}\sum_{t=1}^{5}f\left(x,\left(m_k\right)_t\right)
\end{align}
$$
This equation is more complicated than scenario 1, with the sum over $exp$ function, so the boundary is nonlinear as plot on Figure 2.5.
---

## Ex. 2.3 median-distance-of-sphere

Item | Description
--- | ---
Authors | [maorenxin](https://github.com/maorenxin)
Date | 2015/9/8
Reference | [Wikipedia: Volume of the n-sphere](https://en.wikipedia.org/wiki/Sphere)


**Ex. 2.3** Derive equation (2.24).
$$
\begin{equation}
d(p, N) = \left(\ 1- \frac{1}{2}^{1/N} \right)^{1/p}
\end{equation}
$$

**Proof**

First of all, let $r$ be the median distance from the origin to the closest data point. Then
$$
\begin{equation}
P(\text{All } N \text{ points are further than r from the origin}) = \frac{1}{2}
\end{equation}
$$

Since points $x_i$ are independently distributed, which implies that
$$
\begin{equation}
\frac{1}{2} = \prod_{i=1}^N P\left(\left\| x_i \right\| > r\right)
\end{equation}
$$

and as points $x_i$ are uniformly distributed in the unit ball, we have

$$
\begin{align}
P \left(\left\| x_i \right\| > r \right) &= 1- P\left(\left\| x_i \right\| \leq r \right)\\
&= 1 - \frac{V_p(r)}{V_p(R)}\\
&= 1- r^p
\end{align}
$$
and $V$ represents Volume.

Putting all together, we obtain that
$$
\begin{equation}
\frac{1}{2}=\left(1-r^p \right)^N
\end{equation}
$$

and solving for $r$, we will have
$$
\begin{equation}
d(p, N) = \left(\ 1- \frac{1}{2}^{1/N} \right)^{1/p}
\end{equation}
$$

For the sphere volume of $p$ dimension, we can refer to [Volume of the n-sphere](https://en.wikipedia.org/wiki/Sphere) for more detail. Here we demonstrate the formula:
$$
\begin{align}
V_p({R}) = \frac{R^p}{n} \frac{2\pi^{p/2}}{\Gamma \left(n/2\right)}\\
\Gamma(n/2) = \frac{\left(n-2\right)!!\sqrt{\pi}}{2^{\left(n-1\right)/2}}
\end{align}
$$

---

## Ex. 2.5 linear-model-bias-variance

Item | Description
--- | ---
Authors | [squall1988](https://github.com/squall1988), [PengjuYan](https://github.com/PengjuYan)
Date | 2015/9/10
Reference |[Wikipedia: Bias–variance tradeoff][wikipedia-bias-variance_tradeoff], [Wikipedia: Trace][wikipedia-trace], [Wikipedia: Quadratic form][wikipedia-quadratic-form]

[wikipedia-bias-variance_tradeoff]:https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff
[wikipedia-trace]:https://en.wikipedia.org/wiki/Trace_%28linear_algebra%29
[wikipedia-quadratic-form]:https://en.wikipedia.org/wiki/Quadratic_form_%28statistics%29

**Ex. 2.5**

\(a) Derive equation (2.27). The last line makes use of (3.8) through a
conditioning argument.

\(b) Derive equation (2.28), making use of the _cyclic_ property of the trace
operator $\left[\mathrm{trace}\left(AB\right) = \mathrm{trace}\left(AB\right)\right]$, and its linearity (which allows us
to interchange the order of trace and expectation).

**Proof (a)**

**Step a.1** Let's first prove equation (2.25) the bias-variance decomposition of mean squared error (MSE):

$$
\begin{align}
MSE\left(x_0\right)
&= E_{\mathcal{T}} \left[f\left(x_0\right) - \hat{y}_0\right]^2 \\
&= E_{\mathcal{T}} \left[\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 + \left[E_{\mathcal{T}}\left(\hat{y}_0\right) - f\left(x_0\right)\right]^2 \\
&= Var_{\mathcal{T}}\left(\hat{y}_0\right) + Bias^2\left(\hat{y}_0\right)
\end{align}
$$

It can be shown by the following derivations:

$$
\begin{align}
MSE\left(x_0\right)
&= E_{\mathcal{T}} \left[\hat{y}_0 - f\left(x_0\right)\right]^2 \\
&= E_{\mathcal{T}} \left[\left(\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right) - \left(f\left(x_0\right)- E_{\mathcal{T}}\left(\hat{y}_0\right)\right)\right]^2 \\
&= E_{\mathcal{T}} \left[\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 -2E_{\mathcal{T}} \left[\left(\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right) \left(f\left(x_0\right) - E_{\mathcal{T}}\left(\hat{y}_0\right)\right)\right] + E_{\mathcal{T}} \left[f\left(x_0\right) - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 \\
&= E_{\mathcal{T}} \left[\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 -2 \left(f\left(x_0\right) - E_{\mathcal{T}}\left(\hat{y}_0\right)\right) E_{\mathcal{T}} \left(\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right) + \left[f\left(x_0\right) - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 \\
&= E_{\mathcal{T}} \left[\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 -2 \left(f\left(x_0\right) - E_{\mathcal{T}}\left(\hat{y}_0\right)\right) \times 0 + \left[f\left(x_0\right) - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 \\
&= E_{\mathcal{T}} \left[\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 + \left[f\left(x_0\right) - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 \\
\label{eq:ex-2.5-mse}
&= Var_{\mathcal{T}}\left(\hat{y}_0\right) + Bias^2\left(\hat{y}_0\right)
\end{align}
$$

Note that the bias-variance decomposition of MSE does not depend on any specific assumptions such as linear models or independence.

**Step a.2** Then we show that expected prediction error (EPE) can be decomposed to the irreducible squared error $\sigma^2$ and MSE.

$$
\begin{align}
EPE\left(x_0\right)
&= E_{y_0|x_0}E_{\mathcal{T}} \left[y_0 - \hat{y}_0\right]^2 \\
&= E_{y_0|x_0}E_{\mathcal{T}} \left[\left(y_0 - f\left(x_0\right)\right) - \left(\hat{y}_0- f\left(x_0\right)\right)\right]^2 \\
&= E_{y_0|x_0}E_{\mathcal{T}} \left[y_0 - f\left(x_0\right)\right]^2 -2 E_{y_0|x_0}E_{\mathcal{T}} \left[\left(y_0 - f\left(x_0\right)\right)\left(\hat{y}_0- f\left(x_0\right)\right)\right] + E_{y_0|x_0}E_{\mathcal{T}} \left[\hat{y}_0- f\left(x_0\right)\right]^2 \\
&= E_{y_0|x_0} \left[y_0 - f\left(x_0\right)\right]^2 -2 E_{y_0|x_0} \left(y_0 - f\left(x_0\right)\right) \times E_{\mathcal{T}} \left[\left(\hat{y}_0- f\left(x_0\right)\right)\right] + E_{\mathcal{T}} \left[\hat{y}_0- f\left(x_0\right)\right]^2 \\
&= Var\left(y_0|x_0\right) -2 \times 0 \times E_{\mathcal{T}} \left[\left(\hat{y}_0- f\left(x_0\right)\right)\right] + MSE\left(x_0\right) \\
&= Var\left(y_0|x_0\right) + MSE\left(x_0\right) \\
\end{align}
$$

Also note that in the proof above, the only necessary assumption is that the mean of the noise term $\epsilon$ in $y=f\left(x\right)+\epsilon$ is $0$. We do not need any further assumptions like linear models or unbiased estimation.

Finally we get the EPE equation:

$$
\begin{equation}
\label{eq:ex-2.5-epe}
EPE\left(x_0\right) = Var\left(y_0|x_0\right) + Var_{\mathcal{T}}\left(\hat{y}_0\right) + Bias^2\left(\hat{y}_0\right)
\end{equation}
$$

**Step a.3** Now we need to prove equation (2.27) under the assumptions that $\epsilon$ follows $\mathcal{N}\left(0, \sigma^2\right)$, the relationship between $Y$ and $X$ is linear, and the estimator is a linear regression model.

For a linear regression model, the estimated parameter is given by (see equation(3.6) on page 45):

$$
\begin{align}
\hat{\beta}
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y} \\
&= \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\left(\mathbf{X}\beta + \mathbf{\epsilon}\right) \\
&= \beta + \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon}
\end{align}
$$

Then $\hat{y}_0$ is

$$
\begin{align}
\hat{y}_0
&= x_0^T\hat{\beta} \\
&= x_0^T\beta + x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon}
\end{align}
$$

and $E_{\mathcal{T}}\left(\hat{y}_0\right)$ is

$$
\begin{align}
E_{\mathcal{T}} \left(\hat{y}_0\right)
&= E_{\mathcal{T}} \left(x_0^T\hat{\beta}\right) \\
&= E_{\mathcal{T}} \left(x_0^T\beta\right) + E_{\mathcal{T}} \left[x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon}\right] \\
&= x_0^T\beta + E_{\mathcal{X}}E_{\mathcal{\epsilon}|\mathcal{X}} \left[x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon}\right] \\
&= x_0^T\beta + E_{\mathcal{X}}0 \\
&= x_0^T\beta
\end{align}
$$

Now look at bias and variance of $\hat{y}_0$:

$$
\begin{align}
Bias^2\left(\hat{y}_0\right)
&= \left[E_{\mathcal{T}}\left(\hat{y}_0\right) - f\left(x_0\right)\right]^2 \\
&= \left[x_0^T\beta - x_0^T\beta\right]^2 \\
\label{eq:ex-2.5-bias}
&= 0 \\
Var_{\mathcal{T}}\left(\hat{y}_0\right)
&= E_{\mathcal{T}} \left[\hat{y}_0 - E_{\mathcal{T}}\left(\hat{y}_0\right)\right]^2 \\
&= E_{\mathcal{T}} \left[x_0^T\beta + x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon} - x_0^T\beta\right]^2 \\
&= E_{\mathcal{T}} \left[x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon}\right]^2 \\
&= E_{\mathcal{T}} \left[\left(x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon}\right) \left(x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon}\right)^T\right] \\
&= E_{\mathcal{T}} \left[x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{\epsilon}\mathbf{\epsilon}^T\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}x_0\right] \\
&= E_{\mathcal{T}} \left[x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\sigma^2I_N\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}x_0\right] \\
&= \sigma^2E_{\mathcal{T}} \left[x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{X}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}x_0\right] \\
\label{eq:ex-2.5-variance}
&= \sigma^2E_{\mathcal{T}} \left[x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}x_0\right]^2
\end{align}
$$

Putting $\eqref{eq:ex-2.5-epe}$, $\eqref{eq:ex-2.5-bias}$ and $\eqref{eq:ex-2.5-variance}$ together, we can finally prove equation (2.27):

$$
\begin{align}
EPE\left(x_0\right)
&= Var\left(y_0|x_0\right) + Var_{\mathcal{T}}\left(\hat{y}_0\right) + Bias^2\left(\hat{y}_0\right) \\
&= \sigma^2 + \sigma^2E_{\mathcal{T}} \left[x_0^T\left(\mathbf{X}^T\mathbf{X}\right)^{-1}x_0\right] + 0
\end{align}
$$

**Proof (b)**

In the proof we will use the following properties:

1. If $N$ is large and $\mathcal{T}$ were selected at random, and assuming
$E\left(X\right) = 0$, then $\mathbf{X}^T\mathbf{X} \rightarrow NCov(\mathbf{X})$.
1. Cyclic property of the trace operator: $\mathrm{trace}\left(AB\right) = \mathrm{trace}\left(AB\right)$.
1. Both the trace and expectation operators are linear, so they can be interchanged: $E \circ trace = trace \circ E$.

$$
\begin{align}
E_{x_0}EPE\left(x_0\right)
&\sim \sigma^2/N \cdot E_{x_0}\left[x_0^TCov\left(X\right)^{-1}x_0\right] + \sigma^2 \\
&= \sigma^2/N \cdot E_{x_0}\left[trace\left(x_0^TCov\left(X\right)^{-1}x_0\right)\right] + \sigma^2 \\
&= \sigma^2/N \cdot E_{x_0}\left[trace\left(x_0x_0^TCov\left(X\right)^{-1}\right)\right] + \sigma^2 \\
&= \sigma^2/N \cdot trace\left[E_{x_0}\left(x_0x_0^TCov\left(X\right)^{-1}\right)\right] + \sigma^2 \\
&= \sigma^2/N \cdot trace\left[E_{x_0}\left(x_0x_0^T\right)Cov\left(X\right)^{-1}\right] + \sigma^2 \\
&= \sigma^2/N \cdot trace\left[Cov\left(X\right)Cov\left(X\right)^{-1}\right] + \sigma^2 \\
&= \sigma^2/N \cdot trace I_p + \sigma^2 \\
&= \sigma^2\left(p/N\right) + \sigma^2 \\
\end{align}
$$

---

## Ex. 2.6 weighted-least-squares-problem

Item | Description
--- | ---
Authors | [xdwangkai](https://github.com/xdwangkai)
Date | 2015/9/9
Reference | [Researchgate: A Solution Manual and Notes for: The Elements of Statistical Learning](http://www.researchgate.net/publication/237116664_A_Solution_Manual_and_Notes_for_the_Text_The_Elements_of_Statistical_Learning)

**Ex. 2.6** Consider a regression problem with inputs $x_i$ and outputs $y_i$, and a parameterized model $f_\theta{(x)}$ to be fit by least squares. Show that if there are observations with _tied_ or _identical_ values of $x$, then the fit can be obtained from a  reduced weighted least squares problem.

**Proof**

To fit the model $f_\theta{(x)}$ by least squares, we minimize
$$
\begin{equation}
RSS(\theta)=\sum_{k=1}^N\left(y_k-f_\theta(x_k)\right)^2
\label{eq1}
\end{equation}
$$
as a function of $\theta$.

As _reducible error_ and _irreducible error_ exist, even the _same_ input value of $x$ generate _different_ outputs value of $y$. Denote by $N_u$ the number of _unique_ inputs $x$, and assume that the $i$th $x$ will give $n_i$ different $y$, denote by $y_{ij}$, $1\le j\le n_i$. Rewrite Eq. $\eqref{eq1}$ as
$$
\begin{equation}
RSS(\theta)=\sum_{i=1}^{N_u}\sum_{j=1}^{n_i}\left(y_{ij}-f_\theta(x_i)\right)^2.
\label{eq2}
\end{equation}
$$

By expanding the quadratic in Eq. $\eqref{eq2}$, we have
$$
\begin{align}
RSS(\theta)&= \sum_{i=1}^{N_u}\sum_{j=1}^{n_i}\left(y_{ij}^2-2f_\theta(x_i)y_{ij}+f_\theta(x_i)^2\right)\\
&= \sum_{i=1}^{N_u}n_i\left(\frac{1}{n_i}\sum_{j=1}^{n_i}y_{ij}^2-2f_\theta(x_i)\left(\frac{1}{n_i}\sum_{j=1}^{n_i}y_{ij}\right)+f_\theta(x_i)^2\right)
\label{eq3}
\end{align}
$$

Now we get the _weighted_ item $n_i$. Next, we will rewrite the expressions in the bracket to be consistent with Eq. $\eqref{eq1}$.

In Eq. $\eqref{eq3}$ we notice the term $\frac{1}{n_i}\sum_{j=1}^{n_i}y_{ij}$, which means the average of all responses $y$ resulting from the same input $x_i$. To be briefly expressed, we define $\bar{y_i}=\frac{1}{n_i}\sum_{j=1}^{n_i}y_{ij}$ and rewrite Eq. $\eqref{eq3}$ as
$$
\begin{align}
RSS(\theta)&= \sum_{i=1}^{N_u}n_i\left(\frac{1}{n_i}\sum_{j=1}^{n_i}y_{ij}^2-2f_\theta(x_i)\bar{y_i}+f_\theta(x_i)^2\right)\\
&= \sum_{i=1}^{N_u}n_i\left(\bar{y_i}^2-2f_\theta(x_i)\bar{y_i}+f_\theta(x_i)^2\right)+\sum_{i=1}^{N_u}\sum_{j=1}^{n_i}y_{ij}^2-\sum_{i=1}^{N_u}{n_i\bar{y_i}^2}\\
&= \sum_{i=1}^{N_u}n_i\left(\bar{y_i}-f_\theta(x_i)\right)^2+\sum_{i=1}^{N_u}\sum_{j=1}^{n_i}y_{ij}^2-\sum_{i=1}^{N_u}{n_i\bar{y_i}^2}
\label{eq4}
\end{align}
$$

Once we get the measurements, i.e., the inputs $x_i$ and outputs $y_i$, the expression $\sum_{i=1}^{N_u}\sum_{j=1}^{n_i}y_{ij}^2-\sum_{i=1}^{N_u}{n_i\bar{y_i}^2}$ in Eq. $\eqref{eq4}$ won't change. Thus minimizing Eq. $\eqref{eq1}$ with respect to $\theta$ is equivalent to minimizing:
$$
\begin{equation}
RSS(\theta)=\sum_{i=1}^{N_u}n_i\left(\bar{y_i}-f_\theta(x_i)\right)^2
\end{equation}
\label{eq5}
$$

Now we compare Eq. $\eqref{eq5}$ with Eq. $\eqref{eq1}$:

1. The number of items in Eq. $\eqref{eq5}$ is $N_u$, which is less than the number of inputs value $N$ in Eq. $\eqref{eq1}$. So this problem can be regarded as a _reduced_ one.

2. Each residual error is weighted by $n_i$ in Eq. $\eqref{eq5}$, so this is a _weighted_ least squares problem.

Therefore, the fit of parameterized model $f_\theta{(x)}$ can be obtained from a  reduced weighted least squares problem.

---

## Ex. 2.8 performance-of-lr-and-knn

Item | Description
--- | ---
Authors | [Yang-Zhou](https://github.com/Yang-Zhou)
Date | 2015/9/17

**Ex. 2.8**
Compare the classification performance of linear regression and k–nearest neighbor classification on the zipcode data. In particular, consider only the $2$’s and $3$’s, and $k$ = $1$, $3$, $5$, $7$ and $15$. Show both the training and test error for each choice. The zipcode data are available from the book website http://www-stat.stanford.edu/ElemStatLearn.

**Code**

```{r, eval=FALSE}
# Load knn classifier
library(class)

get.preprocessed <- function(url) {
  # Get file
  temp <- tempfile()
  download.file(url, temp)
  file <- read.table(temp, sep = ' ')
  # filter the dependent variable y
  file2 <- file[file[, 1] %in% c(2, 3), ]
  # filter the independent variable X
  file3 <- file2[, 1:257]
}

lr.predict <- function(model, testset) {
  predset <- predict(model, newdata = testset)
  predset[predset >= 2.5] = 3
  predset[predset < 2.5] = 2
  accuracy <- sum(predset == testset[, 1]) / length(predset)
}

knn.performance <- function(trainset, testset, k) {
  knn.fit <- knn(trainset, testset, cl=trainset[, 1], k=k, prob=F)
  accuracy <- sum(knn.fit == testset[, 1]) / length(knn.fit)
}

# Url of trainset and testset
train.url <- paste('http://statweb.stanford.edu/~tibs/',
                   'ElemStatLearn/datasets/zip.train.gz', sep='')
test.url <- paste('http://statweb.stanford.edu/~tibs/',
                  'ElemStatLearn/datasets/zip.test.gz', sep='')

# Get proprocessed trainset and testset
trainset <- get.preprocessed(train.url)
testset <- get.preprocessed(test.url)

# Performance of Linear Regression
lm.fit <- lm(V1 ~ ., data = trainset)
accuracy.train <- lr.predict(lm.fit, trainset)
accuracy.test <- lr.predict(lm.fit, testset)
sprintf(paste('The accuracy of LR\'s train accuracy is %.3f,',
         'test acccuracy is %.3f'), accuracy.train, accuracy.test)

# Performance of Knn with K = 1, 3, 5, 7, 15
for (i in c(1, 3, 5, 7, 15)) {
  accuracy.train <- knn.performance(trainset, trainset, i)
  accuracy.test <- knn.performance(trainset, testset, i)
  accuracy.str <- sprintf(paste('The accuracy of %2d-nn\'s train',
                    'accuracy is %.3f, test acccuracy is %.3f'),
                    i, accuracy.train, accuracy.test)
  print(accuracy.str)
}
```

The results are:

```
The accuracy of    LR's train accuracy is 0.994, test acccuracy is 0.959
The accuracy of  1-nn's train accuracy is 1.000, test acccuracy is 0.975
The accuracy of  3-nn's train accuracy is 0.996, test acccuracy is 0.970
The accuracy of  5-nn's train accuracy is 0.994, test acccuracy is 0.970
The accuracy of  7-nn's train accuracy is 0.994, test acccuracy is 0.970
The accuracy of 15-nn's train accuracy is 0.991, test acccuracy is 0.962
```