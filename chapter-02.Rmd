---
title: "Overview of Supervised Learning"
output:
  html_document:
    toc: yes
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

---

## Note [p:12 para:2 eq:2.5] derivative-scalar-by-vector

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/8/18
Reference | [Wikipedia: Matrix calculus][wikipedia-matrix-calculus]

[wikipedia-matrix-calculus]: https://en.wikipedia.org/wiki/Matrix_calculus

There are 2 _flavors_ of definition of **the derivative of a scalar by a vector** in terms of the form of the resultant vector:

- Row vector: [Wikipedia: Matrix calculus][wikipedia-matrix-calculus], [tirgul3_derivatives.pdf: Derivatives with respect to vectors](http://www.cs.huji.ac.il/~csip/tirgul3_derivatives.pdf)
- Column vector: [IFEM.AppF.pdf: Matrix Calculus](http://www.colorado.edu/engineering/cas/courses.d/IFEM.d/IFEM.AppF.d/IFEM.AppF.pdf), [onlinelibrary: Differentiation with respect to a vector](http://onlinelibrary.wiley.com/doi/10.1002/0471705195.app3/pdf)

This book follows the latter, i.e., the column vector form:

$$
\begin{equation}
\frac{\partial{y}}{\partial{\mathbf{x}}} = \begin{bmatrix} \frac{\partial y}{\partial x_1} \\ \frac{\partial y}{\partial x_2} \\ \vdots \\ \frac{\partial y}{\partial x_n} \end{bmatrix}
\end{equation}
$$

This can also be verified by equation (3.14) on page 45.

---

## Note [p:15 para:-2] knn-degrees-of-freedom

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/8/20
Reference | [Wikipedia: Degrees of freedom][wikipedia-degrees-of-freedom]

[wikipedia-degrees-of-freedom]: https://en.wikipedia.org/wiki/Degrees_of_freedom_%28statistics%29

In the paragraph of interest, it is said that:

> ... we will see that the _effective_ number of parameters of $k$-nearest neighbors is $N/k$ ... To get an idea of why, note that if the neighborhoods were nonoverlapping, there would be $N/k$ neighborhoods and we would fit one parameter (a mean) in each neighborhood.

In order to understand this paragraph quantitatively, we may need to refer to [Wikipedia: Degrees of freedom][wikipedia-degrees-of-freedom] and equation (3.50) on page 68. In many regression tasks, including $k$-nearest-neighbors, the prediction on training data can be given by a linear combination of target values of the training samples:

$$
\begin{equation}
\hat{\mathbf{y}} = \mathbf{H}\mathbf{y}
\end{equation}
$$

A general definition of the **effective degrees of freedom** is given by the trace of the **hat** matrix $\mathbf{H}$:

$$
\begin{equation}
\mathrm{tr}(\mathbf{H}) = \sum_{i} h_{ii}
\end{equation}
$$

In a $k$-nearest-neighbor fit, each row of the hat matrix $\mathbf{H}$ contains exactly $k$ non-zero cells with the value of $1/k$, and the diagonal cell is always $1/k$ because the nearest neighbor of each data sample is of course itself. Therefore, the trace of the hat matrix is naturally $N/k$.

---

## Note [p:16 para:-1] - [p:17 para:1] simulated-gaussian-mixture

Item | Description
--- | ---
Authors | [PengjuYan](https://github.com/PengjuYan)
Date | 2015/9/1
Reference | [Wikipedia: Mixture model][wikipedia-mixture-model]

[wikipedia-mixture-model]: https://en.wikipedia.org/wiki/Mixture_model

In section 2.3, the training data were _simulated from a model somewhere between the two, but closer to Scenario 2_.

> **Scenario 1**: The training data in each class were generated from bivariate Gaussian distributions with uncorrelated components and different means.

> **Scenario 2**: The training data in each class came from a mixture of 10 low-variance Gaussian distributions, with individual means themselves distributed as Gaussian.

The sampling procedure described in the paragraph of interest **follows** the definition of Gaussian mixture models of no doubt. The difference between that procedure and Scenario 2 lies in the fact that the two means of the Gaussian distributions from which the individual means of the two classes were drawn **are different**, which are $(1, 0)^T$ and $(0, 1)^T$, respectively. In Scenario 2 however, the individual means follow the same Gaussian distribution. Therefore, the separability of the simulated data lies between Scenario 1 and Scenario 2.

---

## Ex 2.1 output-class-index

Item | Description
--- | ---
Authors | [justdark](https://github.com/justdark)
Date | 2015/9/5

**Ex. 2.1** Suppose each of $K$-classes has an associated target $t_{k}$, which is a vector of all zeros, except a one in the $k$th position. Show that classifying to the largest element of $\hat{y}$ amounts to choosing the closest target, $\mathrm{min}_{k}\left|\left|t_{k} - \hat{y}\right|\right|$, if the elements of $\hat{y}$ sum to one.

**Proof**

According to the description, what we need to prove is:
$$
\begin{equation}
\mathop{\mathrm{argmax}}_{k}\left(\hat{y}_k\right)=\mathop{\mathrm{argmin}}_{k}(\left|\left|t_{k}-\hat{y}\right|\right|)
\end{equation}
$$

So we have:
$$
\begin{align}
\mathop{\mathrm{argmin}}_{k}\left(\left|\left|t_{k}-\hat{y}\right|\right|\right) &=  \mathop{\mathrm{argmin}}_{k}\left(t_{k}-\hat{y}\right)^2 \\
&= \mathop{\mathrm{argmin}}_{k} \sum_{i=1}^K\left[\left(t_{k}\right)_{i}-\hat{y}_i\right]^2 \\
&= \mathop{\mathrm{argmin}}_{k} \sum_{i=1}^K\left[\left(t_{k}\right)_{i}^2+\hat{y}_i^2-2\left(t_{k}\right)_{i}\hat{y}_i\right]\\
&= \mathop{\mathrm{argmin}}_{k} \sum_{i=1}^K\left[\left(t_{k}\right)_{i}^2-2\left(t_{k}\right)_{i}\hat{y}_i\right] \\
&= \mathop{\mathrm{argmin}}_{k} \left[\sum_{i=1}^K\left(t_{k}\right)_{i}^2-\sum_{i=1}^K2\left(t_{k}\right)_{i}\hat{y}_i\right] \\
&= \mathop{\mathrm{argmin}}_{k} \left[1-\sum_{i=1}^K2\left(t_{k}\right)_{i}\hat{y}_i\right]\\
&= \mathop{\mathrm{argmax}}_{k} \sum_{i=1}^K\left[\left(t_{k}\right)_{i}\hat{y}_i\right]\\
&= \mathop{\mathrm{argmax}}_{k} \left(\hat{y}_k\right)
\end{align}
$$

As you can see, the last condition that the elements of $\hat{y}$ sum to one doesn't appear in our proof, because it's unnecessary for the conclusion.

---

## Ex. 2.2 bayes-decision-boundary

Item | Description
--- | ---
Authors | [justdark](https://github.com/justdark)
Date | 2015/9/8

**Ex. 2.2** Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5.

**Proof**

From the description we can conclude that the boundary's property:
$$
\begin{equation}
Pr\left(\mathcal{G}_{orange}\left|\right.X=x\right)=Pr\left(\mathcal{G}_{blue}\left|\right.X=x\right)
\end{equation}
$$
where x is the point on the boundary. By the Bayes equation:
$$
\begin{equation}
Pr\left(\mathcal{G}_{k}\left|\right.X = x\right) = \frac{Pr\left(X = x\left|\right.\mathcal{G}_{k}\right)Pr\left(\mathcal{G}_{k}\right)}{Pr\left(X=x\right)}
\end{equation}
$$

Because the $Count\left(orange\right)=Count\left(blue\right)$, so
$$
\begin{equation}
Pr\left(\mathcal{G}_{orange}\right)=Pr\left(\mathcal{G}_{blue}\right)
\end{equation}
$$

We can simplify the boundary's property, it's also **the solution of this exercise**:
$$
\begin{align}
Pr\left(X = x\left|\right.\mathcal{G}_{orange}\right)&=Pr\left(X = x\left|\right.\mathcal{G}_{blue}\right)
\end{align}
$$

How to calculate the $Pr\left(X = x\left|\right.\mathcal{G}_{k}\right)$ ?

For the **Scenario 1**, if the $\mathcal{G}$ is simulated from two bivariate Guassian distribution, we can use the **probability density function (pdf)** of Guassian distribution:
$$
\begin{align}
f\left(x\right)=\frac{1}{\sqrt{2\pi } \sigma}e^{-\frac{\left(x-\mu \right)^{2} }{2\sigma ^{2} }}
\end{align}
$$
Because the $\sigma$ is same for all class, so the boundary is actually **midperpendicular** of line segment from $\mu_{orange}$ to $\mu_{blue}$.

For the **Scenario 2**, each class, we first generate 10 points as the 10 new Guassian distributions' mean, then each distribution generates 10 points as the results, finally we got 100 points for the class.

Use $\left(m_k\right)_t,t<10$ as the results of the first step for class $k$.
The probability that a point is generate from a given mean $\left(m_k\right)_t$ is:
$$
\begin{align}
f\left(x,\left(m_k\right)_t\right)=\frac{1}{\sqrt{2\pi } \sigma^{'}}e^{-\frac{\left(x-\left(m_k\right)_t \right)^{2} }{2\sigma^{'2} } }
\end{align}
$$
(add apostrophe on the $\sigma$ to distinguish it from basic one)

So we can define the probability that the point is generate from the $k$th class as:
$$
\begin{align}
Pr\left(X = x\left|\right.\mathcal{G}_{k}\right) = \frac{1}{10}\sum_{t=1}^{10}f\left(x,\left(m_k\right)_t\right)
\end{align}
$$
This equation is more complicated than scenario 1, with the sum over $exp$ function, so the boundary is nonlinear as plot on Figure 2.5.